{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/worldbank/Pakistan-Poverty-from-Sky/blob/master/DataWork/03_predict_ntl_with_dtl/02_cnn_v3.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "# https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re \n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras import models\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model, Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras as K\n",
    "\n",
    "import logging, os \n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config as cf\n",
    "\n",
    "# Set seeds. Note that using a GPU can still introduce randomness.\n",
    "# (also not taking into account tensorflow randomness)\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        \n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size)) # dtype=int \n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load(os.path.join(NPY_PATH, ID + '.npy'))\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "        \n",
    "        return X, to_categorical(y, num_classes=self.n_classes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_imagenet(height, width, num_classes):\n",
    "    '''\n",
    "    Defines and compiles CNN model.\n",
    "    \n",
    "    Inputs:\n",
    "        height, width, channels, num_classes (int)\n",
    "    Returns:\n",
    "        model (keras.Model object)\n",
    "    '''\n",
    "\n",
    "    # https://medium.com/abraia/first-steps-with-transfer-learning-for-custom-image-classification-with-keras-b941601fcad5\n",
    "    # https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2\n",
    "\n",
    "    #### Base model\n",
    "    input_shape = (height, width, 3)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape, pooling = \"max\")\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #### Model Customization\n",
    "    # We take the last layer of our the model and add it to our classifier\n",
    "    last = base_model.layers[-1].output\n",
    "    x = Flatten()(last)\n",
    "    x = Dense(100, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    # We compile the model\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, training_generator, validation_generator, CNN_MODEL_CHECKPOINT):\n",
    "    '''\n",
    "    Fits model, evaluates model, saves best model over epochs and cross-validations.\n",
    "    \n",
    "    Inputs:\n",
    "        model (CNN model) keras.Model object\n",
    "        trainX, trainY (numpy.ndarray) 4D array of DTL features and 2D array of targets for training\n",
    "        testX, testY (numpy.ndarray) 4D array of DTL features and 2D array of targets for testing\n",
    "        current_kfold (int) iteration in kfold cross-val, default=None for no cross-val\n",
    "        display_metrics (bool) Default=False\n",
    "    Returns:\n",
    "        None\n",
    "    # https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29\n",
    "    '''\n",
    "\n",
    "    # Use early stopping to help with overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=False)\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    mc = ModelCheckpoint(CNN_MODEL_CHECKPOINT, monitor='val_loss', mode='min', \n",
    "                         verbose=True, save_best_only=True)\n",
    "\n",
    "    # Fit model\n",
    "    #history = model.fit(trainX, trainY, \n",
    "    #        epochs=50, \n",
    "    #        batch_size=32, \n",
    "    #        validation_data=(testX, testY), \n",
    "    #        callbacks=[es, mc], \n",
    "    #        verbose=False)\n",
    "    \n",
    "    history = model.fit(x=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        use_multiprocessing=True,\n",
    "                        epochs=50,\n",
    "                        callbacks=[es, mc],\n",
    "                        workers=6)\n",
    "\n",
    "    # Show accuracy\n",
    "    loss, accuracy = model.evaluate(testX, testY, verbose=False)\n",
    "    print(f'                              Accuracy: {accuracy}')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_NAME = 'DHS'\n",
    "SATELLITE = 'l8'\n",
    "BAND = 'BRGB'\n",
    "TARGET_VAR = 'wealth_index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Numpy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of npy files\n",
    "NPY_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "             'Data', \n",
    "             SURVEY_NAME, \n",
    "             'FinalData', \n",
    "             'Individual Datasets',\n",
    "            'cnn_' + SATELLITE,\n",
    "             'npy')\n",
    "\n",
    "NPY_FILES = os.listdir(NPY_PATH)\n",
    "reg = re.compile(r'^' + BAND + '_')                  \n",
    "NPY_FILES = list(filter(reg.search, NPY_FILES)) \n",
    "\n",
    "# List of uids\n",
    "uids = [file.replace('.npy', '').replace(BAND + '_', '') for file in NPY_FILES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load survey data\n",
    "survey_df = pd.read_csv(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData', 'Individual Datasets', 'survey_socioeconomic.csv'))\n",
    "\n",
    "#### Subset survey\n",
    "\n",
    "# Subset if target variable is NA\n",
    "survey_df = survey_df.dropna(axis=0, subset=[TARGET_VAR])\n",
    "\n",
    "# Subset to survey where we have an associated numpy array\n",
    "survey_df = survey_df[survey_df['uid'].isin(uids)]\n",
    "\n",
    "#### Variable Clean/Add\n",
    "\n",
    "# Prep target variable\n",
    "survey_df[TARGET_VAR] = np.round(survey_df[TARGET_VAR]).tolist()\n",
    "survey_df[TARGET_VAR] = survey_df[TARGET_VAR] - 1 # so starts at 0\n",
    "\n",
    "# Add band name\n",
    "survey_df['band_uid'] = BAND + '_' + survey_df['uid']\n",
    "\n",
    "# Indicate Train/Test\n",
    "survey_df['traintest'] = np.random.choice(a = ['train', 'test'], \n",
    "                                      p = [0.8, 0.2],\n",
    "                                      size = survey_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries for Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Dictionary\n",
    "train_uids = survey_df[survey_df.traintest == 'train']['band_uid'].tolist()\n",
    "test_uids = survey_df[survey_df.traintest == 'test']['band_uid'].tolist()\n",
    "\n",
    "partition = {'train': train_uids, \n",
    "             'test': test_uids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dict(zip(survey_df.band_uid, survey_df[TARGET_VAR]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (224,224),\n",
    "          'batch_size': 32,\n",
    "          'n_classes': 5,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['test'], labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_MODEL_PATH = os.path.join('/Users/robmarty/Desktop', f'CNN_DEPVAR.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model_imagenet(params['dim'][0], params['dim'][1], params['n_classes'])\n",
    "evaluate_model(model, training_generator, validation_generator, CNN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
