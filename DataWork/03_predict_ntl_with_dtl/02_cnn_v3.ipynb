{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "02_cnn_v3.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DA6XKs-DESD"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_S9LVMVDESF"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/worldbank/Pakistan-Poverty-from-Sky/blob/master/DataWork/03_predict_ntl_with_dtl/02_cnn_v3.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWjucHU2DESG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deJsZmQSDESG"
      },
      "source": [
        "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "# https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
        "\n",
        "from numpy.random import seed\n",
        "\n",
        "import os, datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re \n",
        "\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model, Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import tensorflow.keras as K\n",
        "\n",
        "import logging, os \n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#import config as cf\n",
        "\n",
        "# Set seeds. Note that using a GPU can still introduce randomness.\n",
        "# (also not taking into account tensorflow randomness)\n",
        "seed(42)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9i801NpDip8",
        "outputId": "194a76fc-d9c3-4a65-f57d-fbf97ac42239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHVApeA8GWGy"
      },
      "source": [
        "GOOGLEDRIVE_DIRECTORY = os.path.join('/content/gdrive/My Drive/World Bank/IEs/Pakistan Poverty Estimation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPRci3qfDESK"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MILqyefDESK"
      },
      "source": [
        "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
        "                 n_classes=10, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' \n",
        "        \n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size)) # dtype=int \n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            X[i,] = np.load(os.path.join(NPY_PATH, ID + '.npy'))\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "        \n",
        "        return X, to_categorical(y, num_classes=self.n_classes)    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c9gwqGADESL"
      },
      "source": [
        "def define_model_imagenet(height, width, num_classes):\n",
        "    '''\n",
        "    Defines and compiles CNN model.\n",
        "    \n",
        "    Inputs:\n",
        "        height, width, channels, num_classes (int)\n",
        "    Returns:\n",
        "        model (keras.Model object)\n",
        "    '''\n",
        "\n",
        "    # https://medium.com/abraia/first-steps-with-transfer-learning-for-custom-image-classification-with-keras-b941601fcad5\n",
        "    # https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2\n",
        "\n",
        "    #### Base model\n",
        "    input_shape = (height, width, 3)\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape, pooling = \"max\")\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    #### Model Customization\n",
        "    # We take the last layer of our the model and add it to our classifier\n",
        "    last = base_model.layers[-1].output\n",
        "    x = Flatten()(last)\n",
        "    x = Dense(100, activation='relu', name='fc1')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "    model = Model(base_model.input, x)\n",
        "    # We compile the model\n",
        "    model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, training_generator, validation_generator, CNN_MODEL_CHECKPOINT):\n",
        "    '''\n",
        "    Fits model, evaluates model, saves best model over epochs and cross-validations.\n",
        "    \n",
        "    Inputs:\n",
        "        model (CNN model) keras.Model object\n",
        "        trainX, trainY (numpy.ndarray) 4D array of DTL features and 2D array of targets for training\n",
        "        testX, testY (numpy.ndarray) 4D array of DTL features and 2D array of targets for testing\n",
        "        current_kfold (int) iteration in kfold cross-val, default=None for no cross-val\n",
        "        display_metrics (bool) Default=False\n",
        "    Returns:\n",
        "        None\n",
        "    # https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29\n",
        "    '''\n",
        "\n",
        "    # Use early stopping to help with overfitting\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=False)\n",
        "\n",
        "    # Save best model based on accuracy\n",
        "    mc = ModelCheckpoint(CNN_MODEL_CHECKPOINT, monitor='val_loss', mode='min', \n",
        "                         verbose=True, save_best_only=True)\n",
        "\n",
        "    # Fit model\n",
        "    #history = model.fit(trainX, trainY, \n",
        "    #        epochs=50, \n",
        "    #        batch_size=32, \n",
        "    #        validation_data=(testX, testY), \n",
        "    #        callbacks=[es, mc], \n",
        "    #        verbose=False)\n",
        "    \n",
        "    history = model.fit(x=training_generator,\n",
        "                        validation_data=validation_generator,\n",
        "                        use_multiprocessing=True,\n",
        "                        epochs=50,\n",
        "                        callbacks=[es, mc],\n",
        "                        workers=6)\n",
        "\n",
        "    # Show accuracy\n",
        "    loss, accuracy = model.evaluate(testX, testY, verbose=False)\n",
        "    print(f'                              Accuracy: {accuracy}')\n",
        "\n",
        "    return history"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnC9uUv6DESR"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb5r2cMgDESS"
      },
      "source": [
        "SURVEY_NAME = 'DHS'\n",
        "SATELLITE = 'l8'\n",
        "BAND = 'BRGB'\n",
        "TARGET_VAR = 'wealth_index'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTatYzPjDEST"
      },
      "source": [
        "### Load Numpy Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0Uxxin3DEST"
      },
      "source": [
        "# List of npy files\n",
        "NPY_PATH = os.path.join(GOOGLEDRIVE_DIRECTORY, \n",
        "             'Data', \n",
        "             SURVEY_NAME, \n",
        "             'FinalData', \n",
        "             'Individual Datasets',\n",
        "            'cnn_' + SATELLITE,\n",
        "             'npy')\n",
        "\n",
        "NPY_FILES = os.listdir(NPY_PATH)\n",
        "reg = re.compile(r'^' + BAND + '_')                  \n",
        "NPY_FILES = list(filter(reg.search, NPY_FILES)) \n",
        "\n",
        "# List of uids\n",
        "uids = [file.replace('.npy', '').replace(BAND + '_', '') for file in NPY_FILES]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRB7jHDoDEST"
      },
      "source": [
        "### Prepare Survey Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br19bGlqDESU"
      },
      "source": [
        "#### Load survey data\n",
        "survey_df = pd.read_csv(os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
        "                                     'Data', \n",
        "                                     SURVEY_NAME, \n",
        "                                     'FinalData', \n",
        "                                     'Individual Datasets', \n",
        "                                     'survey_socioeconomic.csv'))\n",
        "\n",
        "#### Subset survey\n",
        "\n",
        "# Subset if target variable is NA\n",
        "survey_df = survey_df.dropna(axis=0, subset=[TARGET_VAR])\n",
        "\n",
        "# Subset to survey where we have an associated numpy array\n",
        "survey_df = survey_df[survey_df['uid'].isin(uids)]\n",
        "\n",
        "#### Variable Clean/Add\n",
        "\n",
        "# Prep target variable\n",
        "survey_df[TARGET_VAR] = np.round(survey_df[TARGET_VAR]).tolist()\n",
        "survey_df[TARGET_VAR] = survey_df[TARGET_VAR] - 1 # so starts at 0\n",
        "\n",
        "# Add band name\n",
        "survey_df['band_uid'] = BAND + '_' + survey_df['uid']\n",
        "\n",
        "# Indicate Train/Test\n",
        "survey_df['traintest'] = np.random.choice(a = ['train', 'test'], \n",
        "                                      p = [0.8, 0.2],\n",
        "                                      size = survey_df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWunXASZDESU"
      },
      "source": [
        "### Dictionaries for Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dZ6a9jKDESV"
      },
      "source": [
        "# Partition Dictionary\n",
        "train_uids = survey_df[survey_df.traintest == 'train']['band_uid'].tolist()\n",
        "test_uids = survey_df[survey_df.traintest == 'test']['band_uid'].tolist()\n",
        "\n",
        "partition = {'train': train_uids, \n",
        "             'test': test_uids}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfSzkCAjDESV"
      },
      "source": [
        "labels = dict(zip(survey_df.band_uid, survey_df[TARGET_VAR]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD6y2BtyDESV"
      },
      "source": [
        "## Implement CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLJsjkysDESV"
      },
      "source": [
        "# Parameters\n",
        "params = {'dim': (224,224),\n",
        "          'batch_size': 32,\n",
        "          'n_classes': 5,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition['train'], labels, **params)\n",
        "validation_generator = DataGenerator(partition['test'], labels, **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbTR0K7JDESW"
      },
      "source": [
        "CNN_MODEL_PATH = os.path.join('/Users/robmarty/Desktop', f'CNN_DEPVAR.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxcg4gvoDESW"
      },
      "source": [
        "model = define_model_imagenet(params['dim'][0], params['dim'][1], params['n_classes'])\n",
        "evaluate_model(model, training_generator, validation_generator, CNN_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzpJw5iQDESW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isnvb2kjDESX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqqTovbODESX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sGRiFzmDESX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0igrnw6DDESX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQSSdzUfDESX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrH_V9zLDESX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kig8xI3KDESX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zf9GwEUDESX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-3igWDBDESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3GZM2yoDESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxqg6RUhDESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8eiALrqDESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUuSTN1rDESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa4mxIGjDESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I-8xwD3DESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jGOsAnZDESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NilJBRG5DESY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feckQ8HjDESZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFKsG357DESZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW7a55A6DESZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}