{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0MV-G1dXVY2"
   },
   "source": [
    "# Prepare CNN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuOZpPbiXVY5"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=FWksukxasESEiF6lFtnCN5q1JjTlD-yTIMTD_rvm4XU&tc=RO5Kp77fGefx6Eq-wZmSefMGxWhwBCcRt9_kdNKziWk&cc=kKbMd3dqYxh4dKCOLiVk62dMDOITGhMTsGC0039EayU>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=FWksukxasESEiF6lFtnCN5q1JjTlD-yTIMTD_rvm4XU&tc=RO5Kp77fGefx6Eq-wZmSefMGxWhwBCcRt9_kdNKziWk&cc=kKbMd3dqYxh4dKCOLiVk62dMDOITGhMTsGC0039EayU</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AdQt8qhJGcARKBd0monCjyBxWE6I1N0GCUxfeLwRecWb5f-0EEjjStlXNIQ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# USE ACCOUNT: robmarty3@gmail.com\n",
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import config as cf\n",
    "import ee_utils as utils\n",
    "import eeconvert\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "#import geetools\n",
    "#from geetools import ui, cloud_mask\n",
    "#cloud_mask_landsatSR = cloud_mask.landsatSR()\n",
    "#cloud_mask_sentinel2 = cloud_mask.sentinel2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TqWX3QM4mVPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "OUTCOME_VAR = \"viirs\" # \"ntlharmon\" OR \"viirs\"\n",
    "UNDERSAMPLE_INDIA = True\n",
    "SATELLITE = 'landsat' # 's2'; 'landsat'\n",
    "\n",
    "SURVEY_NAME = 'DHS'\n",
    "\n",
    "SKIP_IF_SCRAPED = True\n",
    "CHUNK_SIZE = 1 # Number of observtaions to scrape in GEE at any given time\n",
    "DROPBOX_DIR = cf.DROPBOX_DIRECTORY\n",
    "GOOGLEDRIVE_DIR = cf.GOOGLEDRIVE_DIRECTORY\n",
    "\n",
    "if SATELLITE == 's2':\n",
    "    KERNEL_SIZE = 224\n",
    "elif SATELLITE == 'landsat':\n",
    "    KERNEL_SIZE = 224 #167\n",
    "elif SATELLITE == 'landsat_7':\n",
    "    KERNEL_SIZE = 224 #167\n",
    "\n",
    "print(KERNEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwPA7XTCYmjh",
    "outputId": "74e611ba-5c81-44ca-e2ce-c595399753a0"
   },
   "outputs": [],
   "source": [
    "# Directory to store tfrecords\n",
    "out_path = os.path.join(GOOGLEDRIVE_DIR, \n",
    "            'Data', \n",
    "            SURVEY_NAME, \n",
    "            'FinalData',\n",
    "            'Individual Datasets',\n",
    "            'cnn_' + SATELLITE + '_' + OUTCOME_VAR + '_underia' + str(UNDERSAMPLE_INDIA),\n",
    "            'tfrecords')\n",
    "\n",
    "out_path_errors = os.path.join(GOOGLEDRIVE_DIR, \n",
    "            'Data', \n",
    "            SURVEY_NAME, \n",
    "            'FinalData',\n",
    "            'Individual Datasets',\n",
    "            'cnn_' + SATELLITE + '_' + OUTCOME_VAR + '_underia' + str(UNDERSAMPLE_INDIA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phzXq49zXVY5",
    "outputId": "0b60e419-0e3f-44e2-e927-0895afb316cc"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82424, 9)\n",
      "1    27366\n",
      "2    16543\n",
      "3    15925\n",
      "0    13627\n",
      "4     8963\n",
      "Name: ntl_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "if UNDERSAMPLE_INDIA == True:\n",
    "    UNDERSAMPLE_INDIA_str = \"TRUE\"\n",
    "else:\n",
    "    UNDERSAMPLE_INDIA_str = \"FALSE\"\n",
    "    \n",
    "survey_df = pd.read_csv(os.path.join(DROPBOX_DIR, 'Data', SURVEY_NAME, 'FinalData', 'Individual Datasets', \n",
    "                                     'data_for_cnn_' + OUTCOME_VAR + '_iaunder' + UNDERSAMPLE_INDIA_str + '_' + SATELLITE + '.csv'))\n",
    "\n",
    "### If sentinel, only use most recent\n",
    "if SATELLITE == 's2':\n",
    "    survey_df = survey_df[survey_df.most_recent_survey == True]\n",
    "        \n",
    "### N Observations      \n",
    "print(survey_df.shape)\n",
    "print(survey_df.ntl_group.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "928"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of TF Records\n",
    "tf_record_list = list(np.unique(survey_df.tfrecord_name))\n",
    "\n",
    "len(tf_record_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PTEEJI89XVY6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# If skip already scraped, remove existing tfrecords from tf_record_list\n",
    "if SKIP_IF_SCRAPED:\n",
    "    tf_records_exist = os.listdir(out_path)\n",
    "    tf_record_list = [x for x in tf_record_list if x not in tf_records_exist]\n",
    "    \n",
    "print(len(tf_record_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_record_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_record_list.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_record_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Blank error dataframe\n",
    "errors_df = pd.DataFrame()\n",
    "\n",
    "## Error file name\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "error_file_name = 'errors_' + current_time + '.csv'\n",
    "\n",
    "if True:\n",
    "    ### Loop through all tfrecords\n",
    "    for tfr_i in tf_record_list:\n",
    "\n",
    "        # Sometimes we get computational time out errors. If occurs, just skip and go to next.\n",
    "        # We can then go back and rescrape missed ones.\n",
    "\n",
    "        survey_df_yeari = survey_df[survey_df['tfrecord_name'] == tfr_i]\n",
    "        year_i = survey_df_yeari['year'].iloc[0]\n",
    "\n",
    "        ### Loop through chunks within tfrecord (can only pull so much data from GEE at a time)\n",
    "        survey_df_yeari['chunk_id'] = utils.chunk_ids(survey_df_yeari.shape[0], CHUNK_SIZE)\n",
    "\n",
    "        print(\"Putting \" + str(survey_df_yeari.shape[0]) + \" observations into \" + tfr_i)\n",
    "\n",
    "        proto_examples_all = []\n",
    "        for chunk_i in list(np.unique(survey_df_yeari.chunk_id)):\n",
    "            \n",
    "            try:\n",
    "            \n",
    "                time.sleep(3)\n",
    "                print(\"Observation: \" + str(len(proto_examples_all)) + \"/\" + str(survey_df_yeari.shape[0]))\n",
    "\n",
    "                survey_df_yeari_chunki = survey_df_yeari[survey_df_yeari['chunk_id'] == chunk_i]\n",
    "\n",
    "                proto_examples_i = utils.prep_cnn_np(survey_df_yeari_chunki, SATELLITE, KERNEL_SIZE, year_i)\n",
    "                proto_examples_all.extend(proto_examples_i)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                print(\"Error ---\")\n",
    "                print(survey_df_yeari_chunki['uid'])\n",
    "                \n",
    "                errors_df = errors_df.append(survey_df_yeari_chunki[['uid']], ignore_index = True)\n",
    "                errors_df.to_csv(os.path.join(out_path_errors, error_file_name))\n",
    "                                 \n",
    "                time.sleep(15)\n",
    "                pass\n",
    "\n",
    "        ### Save data as tf record\n",
    "        out_path_i = os.path.join(out_path, tfr_i)\n",
    "        print(out_path_i)\n",
    "        with tf.io.TFRecordWriter(out_path_i) as writer:\n",
    "            for tf_example in proto_examples_all:\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "\n",
    "        print(\"Success \\o/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ### Loop through all tfrecords\n",
    "    for tfr_i in tf_record_list:\n",
    "\n",
    "        # Sometimes we get computational time out errors. If occurs, just skip and go to next.\n",
    "        # We can then go back and rescrape missed ones.\n",
    "        try:\n",
    "\n",
    "            survey_df_yeari = survey_df[survey_df['tfrecord_name'] == tfr_i]\n",
    "            year_i = survey_df_yeari['year'].iloc[0]\n",
    "\n",
    "            ### Loop through chunks within tfrecord (can only pull so much data from GEE at a time)\n",
    "            survey_df_yeari['chunk_id'] = utils.chunk_ids(survey_df_yeari.shape[0], CHUNK_SIZE)\n",
    "\n",
    "            print(\"Putting \" + str(survey_df_yeari.shape[0]) + \" observations into \" + tfr_i)\n",
    "\n",
    "            proto_examples_all = []\n",
    "            for chunk_i in list(np.unique(survey_df_yeari.chunk_id)):\n",
    "                time.sleep(3)\n",
    "                print(\"Observation: \" + str(len(proto_examples_all)) + \"/\" + str(survey_df_yeari.shape[0]))\n",
    "\n",
    "                survey_df_yeari_chunki = survey_df_yeari[survey_df_yeari['chunk_id'] == chunk_i]\n",
    "\n",
    "                proto_examples_i = utils.prep_cnn_np(survey_df_yeari_chunki, SATELLITE, KERNEL_SIZE, year_i)\n",
    "                proto_examples_all.extend(proto_examples_i)\n",
    "\n",
    "            ### Save data as tf record\n",
    "            out_path_i = os.path.join(out_path, tfr_i)\n",
    "            print(out_path_i)\n",
    "            with tf.io.TFRecordWriter(out_path_i) as writer:\n",
    "                for tf_example in proto_examples_all:\n",
    "                    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "            print(\"Success \\o/\")\n",
    "\n",
    "        except:\n",
    "            print(\"Error ---\")\n",
    "            print(survey_df_yeari_chunki['uid'])\n",
    "            time.sleep(15)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = survey_df_yeari_chunki\n",
    "satellite_name = SATELLITE\n",
    "kernel_size = KERNEL_SIZE\n",
    "year = year_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import numpy as np\n",
    "import geetools\n",
    "from geetools import ui, cloud_mask\n",
    "import os, datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "cloud_mask_landsatSR = cloud_mask.landsatSR()\n",
    "cloud_mask_sentinel2 = cloud_mask.sentinel2()\n",
    "\n",
    "# tfrecord helper functions ----------------------------------------------------\n",
    "# https://stackoverflow.com/questions/52324515/passing-multiple-inputs-to-keras-model-from-tf-dataset-api\n",
    "# https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    # If the value is an eager tensor BytesList won't unpack a string from an EagerTensor.\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() \n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def chunk_ids(total_length, chunk_size):\n",
    "    n_numbers = np.ceil(total_length / chunk_size)\n",
    "    n_numbers = int(n_numbers)\n",
    "    \n",
    "    chunk_ids = list(range(0,n_numbers)) * chunk_size\n",
    "    chunk_ids.sort()\n",
    "    chunk_ids = chunk_ids[:total_length]\n",
    "    \n",
    "    return chunk_ids\n",
    "\n",
    "# Main Functions -----------------------------------------------------------------\n",
    "def survey_to_fc(survey_df):\n",
    "    '''\n",
    "    Convert pandas dataframe of survey locations to a feature collection. \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    survey_fc_list = []\n",
    "    \n",
    "    n_rows = survey_df.shape[0]\n",
    "    for i in range(0, n_rows):\n",
    "        survey_df_i = survey_df.iloc[[i]]\n",
    "\n",
    "        f_i = ee.Feature(ee.Geometry.Point([survey_df_i['longitude'].iloc[0], \n",
    "                                            survey_df_i['latitude'].iloc[0]]), \n",
    "                         {'uid': survey_df_i['uid'].iloc[0]})\n",
    "\n",
    "        survey_fc_list.append(f_i)\n",
    "        \n",
    "    survey_fc = ee.FeatureCollection(survey_fc_list)\n",
    "    \n",
    "    return survey_fc\n",
    "\n",
    "def normalized_diff(values1, values2):\n",
    "    '''\n",
    "    Normalized Difference Value\n",
    "\n",
    "    Input:  values1, values2 (must be same dimensions)\n",
    "\n",
    "    Output: np array\n",
    "    '''\n",
    "\n",
    "    return (values2 - values1)/(values2 + values1)\n",
    "\n",
    "def ee_to_np_daytime(daytime_f, survey_df, n_rows, b_b, g_b, r_b): # nir_b, swir_b\n",
    "    '''\n",
    "    Transforms feature collection from neighborhood array to np array. Stacks bands\n",
    "    so that they are: NTL, blue, green, red, NDVI, other single daytime bands\n",
    "\n",
    "    Input:  \n",
    "      f (features)\n",
    "      n_rows (number of features)\n",
    "\n",
    "    Output: np array\n",
    "    '''\n",
    "    \n",
    "    example_proto_list = []\n",
    "\n",
    "    for i in range(0, n_rows):\n",
    "        survey_uid = survey_df['uid'].iloc[i]\n",
    "        #folder_name = survey_df['tf_folder_name'].iloc[i]\n",
    "        viirs_ntl_group = int(survey_df['ntl_group'].iloc[i])\n",
    "        survey_year_i = int(survey_df['year'].iloc[i])\n",
    "        uid_i = survey_df['uid'].iloc[i].encode()\n",
    "        \n",
    "        d_f_i = daytime_f[i]['properties']\n",
    "        #n_f_i = ntl_f[i]['properties']\n",
    "\n",
    "        # SAVE AS TFRECORD\n",
    "\n",
    "        # Prep Files\n",
    "        ### RGB\n",
    "        brgb_l = [np.array(d_f_i[r_b]), np.array(d_f_i[g_b]), np.array(d_f_i[b_b])]\n",
    "        brgb_np = np.stack(brgb_l, axis=-1)\n",
    "        brgb_np = brgb_np.astype(np.uint16)\n",
    "        brgb_np_tf = tf.io.encode_png(brgb_np, compression = 9)\n",
    "        #brgb_np_tf = tf.io.serialize_tensor(brgb_np)\n",
    "        \n",
    "        ### NIR\n",
    "        if False:\n",
    "            bnir_np = d_f_i[nir_b]      \n",
    "            bnir_np = np.expand_dims(bnir_np, axis=2) # original (224, 224), change to (224,224,1) -> so can stack\n",
    "            bnir_np = bnir_np.astype(np.uint16)\n",
    "            bnir_np_tf = tf.io.encode_png(bnir_np, compression = 9)\n",
    "            #bndvi_np_tf = tf.io.serialize_tensor(bndvi_np)\n",
    "\n",
    "        if True:\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/io/encode_png\n",
    "            ### NDVI \n",
    "            bndvi_np = d_f_i['NDVI']      \n",
    "            bndvi_np = np.expand_dims(bndvi_np, axis=2) # original (224, 224), change to (224,224,1) -> so can stack\n",
    "            # Convert from -1 to 1 to 0 to 20000\n",
    "            bndvi_np = bndvi_np + 1\n",
    "            bndvi_np = bndvi_np * 10000\n",
    "            bndvi_np = bndvi_np.astype(np.uint16)\n",
    "            bndvi_np_tf = tf.io.encode_png(bndvi_np, compression = 9)\n",
    "            #bndvi_np_tf = tf.io.serialize_tensor(bndvi_np)\n",
    "\n",
    "            ### BU \n",
    "            bbu_np = d_f_i['BU']      \n",
    "            bbu_np = np.expand_dims(bbu_np, axis=2) # original (224, 224), change to (224,224,1) -> so can stack\n",
    "            # Convert from -1 to 1 to 0 to 20000\n",
    "            bbu_np = bbu_np + 1\n",
    "            bbu_np = bbu_np * 10000\n",
    "            bbu_np = bbu_np.astype(np.uint16)\n",
    "            bbu_np_tf = tf.io.encode_png(bbu_np, compression = 9)\n",
    "            #bndvi_np_tf = tf.io.serialize_tensor(bndvi_np)\n",
    "\n",
    "        ### NTL\n",
    "        # Not uint16, so so serialize\n",
    "        #bntl_np = np.array(n_f_i['avg_rad'])\n",
    "        #bntl_np = np.expand_dims(bntl_np, axis=2)\n",
    "        # Values to uint16\n",
    "        #bntl_np = bntl_np + 2 # Can be negative\n",
    "        #bntl_np = bntl_np * 100 # consider two decimal places before uint16 // could also to * 10 (second decimal may not matter)\n",
    "        #bntl_np[bntl_np >= 65535] = 65535 # within range of uint16\n",
    "        #bntl_np = bntl_np.astype(np.uint16)\n",
    "        #bntl_np_tf = tf.io.encode_png(bntl_np, compression = 9)\n",
    "        #bntl_np_tf = tf.io.serialize_tensor(bntl_np)\n",
    "\n",
    "        ## Create dictionary\n",
    "        feature = {\n",
    "            'uid' : _bytes_feature(uid_i),\n",
    "            'viirs_ntl_group' : _int64_feature(viirs_ntl_group),\n",
    "            'year' : _int64_feature(survey_year_i),\n",
    "            'b_rgb': _bytes_feature(brgb_np_tf),\n",
    "            #'b_nir': _bytes_feature(bnir_np_tf)\n",
    "            'b_ndvi': _bytes_feature(bndvi_np_tf),\n",
    "            'b_bu': _bytes_feature(bbu_np_tf)\n",
    "            }\n",
    "\n",
    "        # Other MS Bands\n",
    "        #b_other_list = []\n",
    "        #for b_other_i in other_bs:\n",
    "        #    bi_np = np.array(d_f_i[b_other_i])\n",
    "        #    bi_np = np.expand_dims(bi_np, axis=2)\n",
    "        #    #bi_np_tf = tf.io.serialize_tensor(bi_np)\n",
    "        #    bi_np = bi_np.astype(np.uint16)\n",
    "        #    bi_np_tf = tf.io.encode_png(bi_np, compression = 9)\n",
    "        #    feature['b_' + b_other_i] = _bytes_feature(bi_np_tf)\n",
    "  \n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "        example_proto_list.append(example_proto)\n",
    "\n",
    "        #out_file_name = os.path.join(out_path, folder_name, survey_uid + '.tfrecord')\n",
    "        #with tf.io.TFRecordWriter(out_file_name) as writer:\n",
    "        #  writer.write(example_proto.SerializeToString())\n",
    "        \n",
    "    return example_proto_list\n",
    "\n",
    "        #bndvi_np = np.expand_dims(bndvi_l, axis=2)\n",
    "        #b_np = np.expand_dims(b_l, axis=2)\n",
    "        #b_np = np.repeat(b_np, 3, -1)\n",
    "        #np.save(os.path.join(out_path, band_i + \"_\" + survey_uid + '.npy'), b_np)\n",
    "        #np.save(os.path.join(out_path, 'BRGB' + \"_\" + survey_uid + '.npy'), brgb_np)\n",
    "        #bndvi_np = np.repeat(bndvi_np, 3, -1)\n",
    "        #np.save(os.path.join(out_path, 'BNDVI' + \"_\" + survey_uid + '.npy'), bndvi_np)\n",
    "\n",
    "        #for band_i in SINGLE_BANDS_ALL:\n",
    "        #    \n",
    "        #    b_l = np.array(f_i[band_i])\n",
    "        #    b_np = np.expand_dims(b_l, axis=2)\n",
    "        #    #b_np = np.repeat(b_np, 3, -1)\n",
    "        #    np.save(os.path.join(out_path, band_i + \"_\" + survey_uid + '.npy'), b_np)\n",
    "\n",
    "    \n",
    "    #return \"Done\"\n",
    "\n",
    "def prep_cnn_np(survey_df,\n",
    "                satellite_name,\n",
    "                kernel_size,\n",
    "                year):\n",
    "    '''\n",
    "    Creates numpy arrays for CNN\n",
    "\n",
    "    Input:  df - pandas dataframe\n",
    "            lat_name - name of latitude variable in df\n",
    "            lon_name - name of longitude variable in df\n",
    "    Output: geopandas dataframe\n",
    "    '''\n",
    "\n",
    "    # Setup --------------------------------------------------------------------\n",
    "    # Survey to FeatureCollection  \n",
    "    survey_fc = survey_to_fc(survey_df)\n",
    "\n",
    "    # Define kernel for neighborhood array\n",
    "    list = ee.List.repeat(1, kernel_size)\n",
    "    lists = ee.List.repeat(list, kernel_size)\n",
    "    kernel = ee.Kernel.fixed(kernel_size, kernel_size, lists)\n",
    "    \n",
    "    # Define satellite\n",
    "    if satellite_name == 's2':\n",
    "        satellite = 's2'\n",
    "    elif satellite_name == 'landsat':\n",
    "        if year >= 2014:\n",
    "            satellite = 'l8'\n",
    "        else:\n",
    "            satellite = 'l7'\n",
    "            \n",
    "    # Define scale\n",
    "    if satellite in ['l7', 'l8']:\n",
    "        SCALE = 30\n",
    "    elif satellite in ['s2']: \n",
    "        SCALE = 10\n",
    "\n",
    "    # Prep NTL -----------------------------------------------------------------\n",
    "    \n",
    "    # Year\n",
    "    # VIIRS starts in 2012. At minimum, use 2013 to have year before and after\n",
    "    #if False:\n",
    "    #    if year <= 2013:\n",
    "    #        year_use = 2013\n",
    "    #    else:\n",
    "    #        year_use = year\n",
    "\n",
    "    #    year_plus = year_use + 1\n",
    "    #    year_minus = year_use - 1\n",
    "\n",
    "    #    year_minus_str = str(year_minus) + '-01-01'\n",
    "    #    year_plus_str = str(year_plus) + '-12-31'\n",
    "\n",
    "        # Reduce image collection\n",
    "    #    ntl_image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "    #        .filterDate(year_minus_str, year_plus_str)\\\n",
    "    #        .median()\n",
    "\n",
    "        # Select Bands  \n",
    "    #    ntl_image = ntl_image.select(['avg_rad'])\n",
    "\n",
    "        # Image to neighborhood array\n",
    "    #    ntl_arrays = ntl_image.neighborhoodToArray(kernel)\n",
    "\n",
    "        # Extract values from GEE    \n",
    "    #    ntl_values_ee = ntl_arrays.sample(\n",
    "    #      region = survey_fc, \n",
    "    #      scale = SCALE,\n",
    "    #      tileScale = 10 #8\n",
    "    #    )\n",
    "\n",
    "    #    ntl_dict_ee = ntl_values_ee.getInfo()\n",
    "\n",
    "        # Convert values to numpy array\n",
    "        #n_rows = survey_df.shape[0]\n",
    "    #    ntl_f = ntl_dict_ee['features']    \n",
    "        \n",
    "    # l7 ----------------------------------------------------------------\n",
    "    if satellite == \"l7\":\n",
    "        \n",
    "        # Bands\n",
    "        b_b = 'B1'\n",
    "        g_b = 'B2' \n",
    "        r_b = 'B3' \n",
    "        nir_b = 'B4'\n",
    "        swir_b = 'B5'\n",
    "        #other_bs = ['B5', 'B6', 'B7']\n",
    "        \n",
    "        #BANDS = single_bs.copy()\n",
    "        BANDS = [b_b].copy()\n",
    "        BANDS.append(g_b)\n",
    "        BANDS.append(r_b)\n",
    "        BANDS.append(nir_b)\n",
    "        BANDS.append(swir_b)\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in May 1999; if year is less than\n",
    "        # 2000, use 2000 as year (to ensure have year before and after)\n",
    "        if year < 2000:\n",
    "            year_use = 2000\n",
    "        else:\n",
    "            year_use = year\n",
    "\n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median() #\\\n",
    "            #.multiply(0.0001)\n",
    "    \n",
    "    # l8 ----------------------------------------------------------------\n",
    "    if satellite == \"l8\":\n",
    "                \n",
    "        # Bands\n",
    "        # FOR COLLECTION 2\n",
    "        #b_b = 'SR_B2'\n",
    "        #g_b = 'SR_B3' \n",
    "        #r_b = 'SR_B4' \n",
    "        #nir_b = 'SR_B5'\n",
    "        #other_bs = ['SR_B6', 'SR_B7', 'ST_B10']\n",
    "        \n",
    "        # FOR COLLECTION 1\n",
    "        b_b = 'B2'\n",
    "        g_b = 'B3' \n",
    "        r_b = 'B4' \n",
    "        nir_b = 'B5'\n",
    "        swir_b = 'B6'\n",
    "        #other_bs = ['B6', 'B7', 'B10']\n",
    "        \n",
    "        #BANDS = single_bs.copy()\n",
    "        BANDS = [b_b].copy()\n",
    "        BANDS.append(g_b)\n",
    "        BANDS.append(r_b)\n",
    "        BANDS.append(nir_b)\n",
    "        BANDS.append(swir_b)\n",
    "\n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        #image = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\\\n",
    "        #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "        #    #.map(cloud_mask_landsatSR)\\ #TODO cloud_mask_landsatSR doesn't work with landsat collection 2\n",
    "        #    .median() #\\\n",
    "        #    #.multiply(0.0001)\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\n",
    "        \n",
    "        #image = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\\\n",
    "        #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "        #    .median()\n",
    "            \n",
    "    # s2 ----------------------------------------------------------------\n",
    "    if satellite == \"s2\":\n",
    "        \n",
    "        # Bands\n",
    "        b_b = 'B2'\n",
    "        g_b = 'B3' \n",
    "        r_b = 'B4' \n",
    "        nir_b = 'B8'\n",
    "        swir_b = 'B11'\n",
    "        #other_bs = ['B5', 'B6', 'B7', 'B8A', 'B11', 'B12', 'AOT']\n",
    "     \n",
    "        #BANDS = single_bs.copy()\n",
    "        BANDS = [b_b].copy()\n",
    "        BANDS.append(g_b)\n",
    "        BANDS.append(r_b)\n",
    "        BANDS.append(nir_b)\n",
    "        BANDS.append(swir_b)\n",
    "        \n",
    "        # Year\n",
    "        # sentinel starts in March 2017; juse use 2018\n",
    "        year_use = 2019\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "\n",
    "        # Number of bands changes in sentinel, so need to select here before aggregate\n",
    "        # https://gis.stackexchange.com/questions/374010/gee-tile-error-expected-a-homogeneous-image-collection-but-an-image-with-incom\n",
    "        image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_sentinel2)\\\n",
    "            .select(BANDS)\\\n",
    "            .median() # \\\n",
    "            #.multiply(0.0001)\n",
    "\n",
    "    # Select Bands\n",
    "    image = image.select(BANDS)\n",
    "    \n",
    "    # Create Indices\n",
    "    # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "    ndvi = image.normalizedDifference([nir_b, r_b]).rename('NDVI');\n",
    "    ndbi = image.normalizedDifference([swir_b, nir_b]).rename('NDBI');\n",
    "    image = image.addBands(ndvi)\n",
    "    image = image.addBands(ndbi)\n",
    "        \n",
    "    bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "    image = image.addBands(bu)\n",
    "        \n",
    "    # Subset bands; don't need those used to create NDVI and NDBI\n",
    "    image = image.select([b_b, g_b, r_b, 'NDVI', 'BU'])\n",
    "        \n",
    "    # Image to neighborhood array\n",
    "    arrays = image.neighborhoodToArray(kernel)\n",
    "    \n",
    "    # New ---------\n",
    "    #neighborhoodImage = myImageToBeSampled.neighborhoodToArray(kernel)\n",
    "    #samples = arrays.sampleRegions(collection=survey_fc)\n",
    "    \n",
    "    # ee.batch.Export.table.toCloudStorage\n",
    "    # ee.batch.Export.table.toDrive\n",
    "    # Export.table.toDrive\n",
    "    #mytask = ee.batch.Export.table.toDrive(\n",
    "    # collection = samples,\n",
    "    # fileFormat = 'TFRecord',\n",
    "    # description = 'test123',\n",
    "    # folder = 'gee_extracts',\n",
    "    # selectors = [b_b, g_b, r_b, 'NDVI', 'BU'] + ['uid', 'ntl_group'])\n",
    "    \n",
    "    #return mytask\n",
    "\n",
    "    # OLD ---------\n",
    "    # Extract values from GEE   \n",
    "    values_ee = arrays.sample(\n",
    "      region = survey_fc, \n",
    "      scale = SCALE,\n",
    "      tileScale = 12 # 8\n",
    "    )\n",
    "    \n",
    "    dict_ee = values_ee.getInfo()\n",
    "     \n",
    "    # Convert values to numpy array\n",
    "    n_rows = survey_df.shape[0]\n",
    "    daytime_f = dict_ee['features']\n",
    "    \n",
    "    # Extract data\n",
    "    out_ex_proto_list = ee_to_np_daytime(daytime_f, survey_df, n_rows, b_b, g_b, r_b)\n",
    "    \n",
    "    return out_ex_proto_list\n",
    "\n",
    "# https://csaybar.github.io/blog/2019/05/30/eetf/\n",
    "# https://stackoverflow.com/questions/63000565/extract-10000-images-from-google-earth-engine\n",
    "# https://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/UNET_regression_demo.ipynb#scrollTo=-IlgXu-vcUEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.io' has no attribute 'encode_png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m1/8h14xfm56hd6qfgz6btm1rd80000gn/T/ipykernel_56214/2007812258.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;31m# Extract data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m \u001b[0mout_ex_proto_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mee_to_np_daytime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaytime_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurvey_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/m1/8h14xfm56hd6qfgz6btm1rd80000gn/T/ipykernel_56214/1365876260.py\u001b[0m in \u001b[0;36mee_to_np_daytime\u001b[0;34m(daytime_f, survey_df, n_rows, b_b, g_b, r_b)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mbrgb_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrgb_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mbrgb_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrgb_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mbrgb_np_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrgb_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;31m#brgb_np_tf = tf.io.serialize_tensor(brgb_np)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.io' has no attribute 'encode_png'"
     ]
    }
   ],
   "source": [
    "# Setup --------------------------------------------------------------------\n",
    "# Survey to FeatureCollection  \n",
    "survey_fc = survey_to_fc(survey_df)\n",
    "\n",
    "# Define kernel for neighborhood array\n",
    "list = ee.List.repeat(1, kernel_size)\n",
    "lists = ee.List.repeat(list, kernel_size)\n",
    "kernel = ee.Kernel.fixed(kernel_size, kernel_size, lists)\n",
    "\n",
    "# Define satellite\n",
    "if satellite_name == 's2':\n",
    "    satellite = 's2'\n",
    "elif satellite_name == 'landsat':\n",
    "    if year >= 2014:\n",
    "        satellite = 'l8'\n",
    "    else:\n",
    "        satellite = 'l7'\n",
    "\n",
    "# Define scale\n",
    "if satellite in ['l7', 'l8']:\n",
    "    SCALE = 30\n",
    "elif satellite in ['s2']: \n",
    "    SCALE = 10\n",
    "    \n",
    "# l7 ----------------------------------------------------------------\n",
    "if satellite == \"l7\":\n",
    "\n",
    "    # Bands\n",
    "    b_b = 'B1'\n",
    "    g_b = 'B2' \n",
    "    r_b = 'B3' \n",
    "    nir_b = 'B4'\n",
    "    swir_b = 'B5'\n",
    "    #other_bs = ['B5', 'B6', 'B7']\n",
    "\n",
    "    #BANDS = single_bs.copy()\n",
    "    BANDS = [b_b].copy()\n",
    "    BANDS.append(g_b)\n",
    "    BANDS.append(r_b)\n",
    "    BANDS.append(nir_b)\n",
    "    BANDS.append(swir_b)\n",
    "\n",
    "    # Year\n",
    "    # landsat 8 starts in May 1999; if year is less than\n",
    "    # 2000, use 2000 as year (to ensure have year before and after)\n",
    "    if year < 2000:\n",
    "        year_use = 2000\n",
    "    else:\n",
    "        year_use = year\n",
    "\n",
    "    # Year\n",
    "    year_use = year\n",
    "\n",
    "    year_plus = year_use + 1\n",
    "    year_minus = year_use - 1\n",
    "\n",
    "    year_minus_str = str(year_minus) + '-01-01'\n",
    "    year_plus_str = str(year_plus) + '-12-31'\n",
    "\n",
    "    image = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\\\n",
    "        .filterDate(year_minus_str, year_plus_str)\\\n",
    "        .map(cloud_mask_landsatSR)\\\n",
    "        .median() #\\\n",
    "        #.multiply(0.0001)\n",
    "\n",
    "# l8 ----------------------------------------------------------------\n",
    "if satellite == \"l8\":\n",
    "\n",
    "    # Bands\n",
    "    # FOR COLLECTION 2\n",
    "    #b_b = 'SR_B2'\n",
    "    #g_b = 'SR_B3' \n",
    "    #r_b = 'SR_B4' \n",
    "    #nir_b = 'SR_B5'\n",
    "    #other_bs = ['SR_B6', 'SR_B7', 'ST_B10']\n",
    "\n",
    "    # FOR COLLECTION 1\n",
    "    b_b = 'B2'\n",
    "    g_b = 'B3' \n",
    "    r_b = 'B4' \n",
    "    nir_b = 'B5'\n",
    "    swir_b = 'B6'\n",
    "    #other_bs = ['B6', 'B7', 'B10']\n",
    "\n",
    "    #BANDS = single_bs.copy()\n",
    "    BANDS = [b_b].copy()\n",
    "    BANDS.append(g_b)\n",
    "    BANDS.append(r_b)\n",
    "    BANDS.append(nir_b)\n",
    "    BANDS.append(swir_b)\n",
    "\n",
    "    # Year\n",
    "    # landsat 8 starts in April 2013; if year is less than\n",
    "    # 2014, use 2014 as year (to ensure have year before and after)\n",
    "    if year < 2014:\n",
    "        year_use = 2014\n",
    "    else:\n",
    "        year_use = year\n",
    "\n",
    "    year_plus = year_use + 1\n",
    "    year_minus = year_use - 1\n",
    "\n",
    "    year_minus_str = str(year_minus) + '-01-01'\n",
    "    year_plus_str = str(year_plus) + '-12-31'\n",
    "\n",
    "    #image = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\\\n",
    "    #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "    #    #.map(cloud_mask_landsatSR)\\ #TODO cloud_mask_landsatSR doesn't work with landsat collection 2\n",
    "    #    .median() #\\\n",
    "    #    #.multiply(0.0001)\n",
    "\n",
    "    image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "        .filterDate(year_minus_str, year_plus_str)\\\n",
    "        .map(cloud_mask_landsatSR)\\\n",
    "        .median()\n",
    "\n",
    "    #image = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\\\n",
    "    #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "    #    .median()\n",
    "\n",
    "# s2 ----------------------------------------------------------------\n",
    "if satellite == \"s2\":\n",
    "\n",
    "    # Bands\n",
    "    b_b = 'B2'\n",
    "    g_b = 'B3' \n",
    "    r_b = 'B4' \n",
    "    nir_b = 'B8'\n",
    "    swir_b = 'B11'\n",
    "    #other_bs = ['B5', 'B6', 'B7', 'B8A', 'B11', 'B12', 'AOT']\n",
    "\n",
    "    #BANDS = single_bs.copy()\n",
    "    BANDS = [b_b].copy()\n",
    "    BANDS.append(g_b)\n",
    "    BANDS.append(r_b)\n",
    "    BANDS.append(nir_b)\n",
    "    BANDS.append(swir_b)\n",
    "\n",
    "    # Year\n",
    "    # sentinel starts in March 2017; juse use 2018\n",
    "    year_use = 2019\n",
    "\n",
    "    year_plus = year_use + 1\n",
    "    year_minus = year_use - 1\n",
    "\n",
    "    year_minus_str = str(year_minus) + '-01-01'\n",
    "    year_plus_str = str(year_plus) + '-12-31'\n",
    "\n",
    "    # Number of bands changes in sentinel, so need to select here before aggregate\n",
    "    # https://gis.stackexchange.com/questions/374010/gee-tile-error-expected-a-homogeneous-image-collection-but-an-image-with-incom\n",
    "    image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "        .filterDate(year_minus_str, year_plus_str)\\\n",
    "        .map(cloud_mask_sentinel2)\\\n",
    "        .select(BANDS)\\\n",
    "        .median() # \\\n",
    "        #.multiply(0.0001)\n",
    "\n",
    "# Select Bands\n",
    "image = image.select(BANDS)\n",
    "\n",
    "# Create Indices\n",
    "# https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "ndvi = image.normalizedDifference([nir_b, r_b]).rename('NDVI');\n",
    "ndbi = image.normalizedDifference([swir_b, nir_b]).rename('NDBI');\n",
    "image = image.addBands(ndvi)\n",
    "image = image.addBands(ndbi)\n",
    "\n",
    "bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "image = image.addBands(bu)\n",
    "\n",
    "# Subset bands; don't need those used to create NDVI and NDBI\n",
    "image = image.select([b_b, g_b, r_b, 'NDVI', 'BU'])\n",
    "\n",
    "# Image to neighborhood array\n",
    "arrays = image.neighborhoodToArray(kernel)\n",
    "\n",
    "# New ---------\n",
    "#neighborhoodImage = myImageToBeSampled.neighborhoodToArray(kernel)\n",
    "#samples = arrays.sampleRegions(collection=survey_fc)\n",
    "\n",
    "# ee.batch.Export.table.toCloudStorage\n",
    "# ee.batch.Export.table.toDrive\n",
    "# Export.table.toDrive\n",
    "#mytask = ee.batch.Export.table.toDrive(\n",
    "# collection = samples,\n",
    "# fileFormat = 'TFRecord',\n",
    "# description = 'test123',\n",
    "# folder = 'gee_extracts',\n",
    "# selectors = [b_b, g_b, r_b, 'NDVI', 'BU'] + ['uid', 'ntl_group'])\n",
    "\n",
    "#return mytask\n",
    "\n",
    "# OLD ---------\n",
    "# Extract values from GEE   \n",
    "values_ee = arrays.sample(\n",
    "  region = survey_fc, \n",
    "  scale = SCALE,\n",
    "  tileScale = 12 # 8\n",
    ")\n",
    "\n",
    "dict_ee = values_ee.getInfo()\n",
    "\n",
    "# Convert values to numpy array\n",
    "n_rows = survey_df.shape[0]\n",
    "daytime_f = dict_ee['features']\n",
    "\n",
    "# Extract data\n",
    "out_ex_proto_list = ee_to_np_daytime(daytime_f, survey_df, n_rows, b_b, g_b, r_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_cnn_np(survey_df,\n",
    "                satellite_name,\n",
    "                kernel_size,\n",
    "                year):\n",
    "    '''\n",
    "    Creates numpy arrays for CNN\n",
    "\n",
    "    Input:  df - pandas dataframe\n",
    "            lat_name - name of latitude variable in df\n",
    "            lon_name - name of longitude variable in df\n",
    "    Output: geopandas dataframe\n",
    "    '''\n",
    "\n",
    "    # Setup --------------------------------------------------------------------\n",
    "    # Survey to FeatureCollection  \n",
    "    survey_fc = survey_to_fc(survey_df)\n",
    "\n",
    "    # Define kernel for neighborhood array\n",
    "    list = ee.List.repeat(1, kernel_size)\n",
    "    lists = ee.List.repeat(list, kernel_size)\n",
    "    kernel = ee.Kernel.fixed(kernel_size, kernel_size, lists)\n",
    "    \n",
    "    # Define satellite\n",
    "    if satellite_name == 's2':\n",
    "        satellite = 's2'\n",
    "    elif satellite_name == 'landsat':\n",
    "        if year >= 2014:\n",
    "            satellite = 'l8'\n",
    "        else:\n",
    "            satellite = 'l7'\n",
    "            \n",
    "    # Define scale\n",
    "    if satellite in ['l7', 'l8']:\n",
    "        SCALE = 30\n",
    "    elif satellite in ['s2']: \n",
    "        SCALE = 10\n",
    "\n",
    "    # Prep NTL -----------------------------------------------------------------\n",
    "    \n",
    "    # Year\n",
    "    # VIIRS starts in 2012. At minimum, use 2013 to have year before and after\n",
    "    #if False:\n",
    "    #    if year <= 2013:\n",
    "    #        year_use = 2013\n",
    "    #    else:\n",
    "    #        year_use = year\n",
    "\n",
    "    #    year_plus = year_use + 1\n",
    "    #    year_minus = year_use - 1\n",
    "\n",
    "    #    year_minus_str = str(year_minus) + '-01-01'\n",
    "    #    year_plus_str = str(year_plus) + '-12-31'\n",
    "\n",
    "        # Reduce image collection\n",
    "    #    ntl_image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "    #        .filterDate(year_minus_str, year_plus_str)\\\n",
    "    #        .median()\n",
    "\n",
    "        # Select Bands  \n",
    "    #    ntl_image = ntl_image.select(['avg_rad'])\n",
    "\n",
    "        # Image to neighborhood array\n",
    "    #    ntl_arrays = ntl_image.neighborhoodToArray(kernel)\n",
    "\n",
    "        # Extract values from GEE    \n",
    "    #    ntl_values_ee = ntl_arrays.sample(\n",
    "    #      region = survey_fc, \n",
    "    #      scale = SCALE,\n",
    "    #      tileScale = 10 #8\n",
    "    #    )\n",
    "\n",
    "    #    ntl_dict_ee = ntl_values_ee.getInfo()\n",
    "\n",
    "        # Convert values to numpy array\n",
    "        #n_rows = survey_df.shape[0]\n",
    "    #    ntl_f = ntl_dict_ee['features']    \n",
    "        \n",
    "    # l7 ----------------------------------------------------------------\n",
    "    if satellite == \"l7\":\n",
    "        \n",
    "        # Bands\n",
    "        b_b = 'B1'\n",
    "        g_b = 'B2' \n",
    "        r_b = 'B3' \n",
    "        nir_b = 'B4'\n",
    "        swir_b = 'B5'\n",
    "        #other_bs = ['B5', 'B6', 'B7']\n",
    "        \n",
    "        #BANDS = single_bs.copy()\n",
    "        BANDS = [b_b].copy()\n",
    "        BANDS.append(g_b)\n",
    "        BANDS.append(r_b)\n",
    "        BANDS.append(nir_b)\n",
    "        BANDS.append(swir_b)\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in May 1999; if year is less than\n",
    "        # 2000, use 2000 as year (to ensure have year before and after)\n",
    "        if year < 2000:\n",
    "            year_use = 2000\n",
    "        else:\n",
    "            year_use = year\n",
    "\n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median() #\\\n",
    "            #.multiply(0.0001)\n",
    "    \n",
    "    # l8 ----------------------------------------------------------------\n",
    "    if satellite == \"l8\":\n",
    "                \n",
    "        # Bands\n",
    "        # FOR COLLECTION 2\n",
    "        #b_b = 'SR_B2'\n",
    "        #g_b = 'SR_B3' \n",
    "        #r_b = 'SR_B4' \n",
    "        #nir_b = 'SR_B5'\n",
    "        #other_bs = ['SR_B6', 'SR_B7', 'ST_B10']\n",
    "        \n",
    "        # FOR COLLECTION 1\n",
    "        b_b = 'B2'\n",
    "        g_b = 'B3' \n",
    "        r_b = 'B4' \n",
    "        nir_b = 'B5'\n",
    "        swir_b = 'B6'\n",
    "        #other_bs = ['B6', 'B7', 'B10']\n",
    "        \n",
    "        #BANDS = single_bs.copy()\n",
    "        BANDS = [b_b].copy()\n",
    "        BANDS.append(g_b)\n",
    "        BANDS.append(r_b)\n",
    "        BANDS.append(nir_b)\n",
    "        BANDS.append(swir_b)\n",
    "\n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        #image = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\\\n",
    "        #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "        #    #.map(cloud_mask_landsatSR)\\ #TODO cloud_mask_landsatSR doesn't work with landsat collection 2\n",
    "        #    .median() #\\\n",
    "        #    #.multiply(0.0001)\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\n",
    "        \n",
    "        #image = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\\\n",
    "        #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "        #    .median()\n",
    "            \n",
    "    # s2 ----------------------------------------------------------------\n",
    "    if satellite == \"s2\":\n",
    "        \n",
    "        # Bands\n",
    "        b_b = 'B2'\n",
    "        g_b = 'B3' \n",
    "        r_b = 'B4' \n",
    "        nir_b = 'B8'\n",
    "        swir_b = 'B11'\n",
    "        #other_bs = ['B5', 'B6', 'B7', 'B8A', 'B11', 'B12', 'AOT']\n",
    "     \n",
    "        #BANDS = single_bs.copy()\n",
    "        BANDS = [b_b].copy()\n",
    "        BANDS.append(g_b)\n",
    "        BANDS.append(r_b)\n",
    "        BANDS.append(nir_b)\n",
    "        BANDS.append(swir_b)\n",
    "        \n",
    "        # Year\n",
    "        # sentinel starts in March 2017; juse use 2018\n",
    "        year_use = 2019\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "\n",
    "        # Number of bands changes in sentinel, so need to select here before aggregate\n",
    "        # https://gis.stackexchange.com/questions/374010/gee-tile-error-expected-a-homogeneous-image-collection-but-an-image-with-incom\n",
    "        image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_sentinel2)\\\n",
    "            .select(BANDS)\\\n",
    "            .median() # \\\n",
    "            #.multiply(0.0001)\n",
    "\n",
    "    # Select Bands\n",
    "    image = image.select(BANDS)\n",
    "    \n",
    "    # Create Indices\n",
    "    # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "    ndvi = image.normalizedDifference([nir_b, r_b]).rename('NDVI');\n",
    "    ndbi = image.normalizedDifference([swir_b, nir_b]).rename('NDBI');\n",
    "    image = image.addBands(ndvi)\n",
    "    image = image.addBands(ndbi)\n",
    "        \n",
    "    bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "    image = image.addBands(bu)\n",
    "        \n",
    "    # Subset bands; don't need those used to create NDVI and NDBI\n",
    "    image = image.select([b_b, g_b, r_b, 'NDVI', 'BU'])\n",
    "        \n",
    "    # Image to neighborhood array\n",
    "    arrays = image.neighborhoodToArray(kernel)\n",
    "    \n",
    "    # New ---------\n",
    "    #neighborhoodImage = myImageToBeSampled.neighborhoodToArray(kernel)\n",
    "    #samples = arrays.sampleRegions(collection=survey_fc)\n",
    "    \n",
    "    # ee.batch.Export.table.toCloudStorage\n",
    "    # ee.batch.Export.table.toDrive\n",
    "    # Export.table.toDrive\n",
    "    #mytask = ee.batch.Export.table.toDrive(\n",
    "    # collection = samples,\n",
    "    # fileFormat = 'TFRecord',\n",
    "    # description = 'test123',\n",
    "    # folder = 'gee_extracts',\n",
    "    # selectors = [b_b, g_b, r_b, 'NDVI', 'BU'] + ['uid', 'ntl_group'])\n",
    "    \n",
    "    #return mytask\n",
    "\n",
    "    # OLD ---------\n",
    "    # Extract values from GEE   \n",
    "    values_ee = arrays.sample(\n",
    "      region = survey_fc, \n",
    "      scale = SCALE,\n",
    "      tileScale = 12 # 8\n",
    "    )\n",
    "    \n",
    "    dict_ee = values_ee.getInfo()\n",
    "     \n",
    "    # Convert values to numpy array\n",
    "    n_rows = survey_df.shape[0]\n",
    "    daytime_f = dict_ee['features']\n",
    "    \n",
    "    # Extract data\n",
    "    out_ex_proto_list = ee_to_np_daytime(daytime_f, survey_df, n_rows, b_b, g_b, r_b)\n",
    "    \n",
    "    return out_ex_proto_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'encode_png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m1/8h14xfm56hd6qfgz6btm1rd80000gn/T/ipykernel_56214/3462283977.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbndvi_np_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbndvi_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'encode_png'"
     ]
    }
   ],
   "source": [
    "bndvi_np_tf = tf.io.encode_png(bndvi_np, compression = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bndvi_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m1/8h14xfm56hd6qfgz6btm1rd80000gn/T/ipykernel_56214/333035388.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbndvi_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bndvi_np' is not defined"
     ]
    }
   ],
   "source": [
    "tf.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robmarty/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putting 66 observations into forcnn_AL_1_1_all.tfrecord\n",
      "Observation: 0/66\n",
      "Observation: 1/66\n",
      "Observation: 2/66\n",
      "Observation: 3/66\n",
      "Error!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-35e4ef9beeda>\", line 32, in <module>\n",
      "    proto_examples_i = utils.prep_cnn_np(survey_df_yeari_chunki, SATELLITE, KERNEL_SIZE, year_i)\n",
      "  File \"/Users/robmarty/Documents/Github/Pakistan-Poverty-from-Sky/DataWork/02_get_process_ancillary_data/CNN Features Predict NTL/ee_utils.py\", line 443, in prep_cnn_np\n",
      "    dict_ee = values_ee.getInfo()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/collection.py\", line 127, in getInfo\n",
      "    return super(Collection, self).getInfo()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/computedobject.py\", line 98, in getInfo\n",
      "    return data.computeValue(self)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/data.py\", line 678, in computeValue\n",
      "    prettyPrint=False))['result']\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/data.py\", line 334, in _execute_cloud_call\n",
      "    return call.execute(num_retries=num_retries)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/_helpers.py\", line 134, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/http.py\", line 909, in execute\n",
      "    headers=self.headers,\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/http.py\", line 177, in _retry_request\n",
      "    resp, content = http.request(uri, method, *args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/google_auth_httplib2.py\", line 198, in request\n",
      "    uri, method, body=body, headers=request_headers, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2/__init__.py\", line 1726, in request\n",
      "    conn, authority, uri, request_uri, method, body, headers, redirections, cachekey,\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2/__init__.py\", line 1441, in _request\n",
      "    (response, content) = self._conn_request(conn, request_uri, method, body, headers)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2shim/__init__.py\", line 148, in _conn_request\n",
      "    decode_content=decode)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 72, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 150, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\", line 324, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 1321, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 296, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 257, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/ssl.py\", line 1052, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/ssl.py\", line 911, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-35e4ef9beeda>\", line 39, in <module>\n",
      "    time.sleep(10)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 684, in getsourcefile\n",
      "    filename = getfile(object)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-35e4ef9beeda>\", line 32, in <module>\n",
      "    proto_examples_i = utils.prep_cnn_np(survey_df_yeari_chunki, SATELLITE, KERNEL_SIZE, year_i)\n",
      "  File \"/Users/robmarty/Documents/Github/Pakistan-Poverty-from-Sky/DataWork/02_get_process_ancillary_data/CNN Features Predict NTL/ee_utils.py\", line 443, in prep_cnn_np\n",
      "    dict_ee = values_ee.getInfo()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/collection.py\", line 127, in getInfo\n",
      "    return super(Collection, self).getInfo()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/computedobject.py\", line 98, in getInfo\n",
      "    return data.computeValue(self)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/data.py\", line 678, in computeValue\n",
      "    prettyPrint=False))['result']\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/data.py\", line 334, in _execute_cloud_call\n",
      "    return call.execute(num_retries=num_retries)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/_helpers.py\", line 134, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/http.py\", line 909, in execute\n",
      "    headers=self.headers,\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/http.py\", line 177, in _retry_request\n",
      "    resp, content = http.request(uri, method, *args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/google_auth_httplib2.py\", line 198, in request\n",
      "    uri, method, body=body, headers=request_headers, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2/__init__.py\", line 1726, in request\n",
      "    conn, authority, uri, request_uri, method, body, headers, redirections, cachekey,\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2/__init__.py\", line 1441, in _request\n",
      "    (response, content) = self._conn_request(conn, request_uri, method, body, headers)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2shim/__init__.py\", line 148, in _conn_request\n",
      "    decode_content=decode)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 72, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 150, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\", line 324, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 1321, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 296, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 257, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/ssl.py\", line 1052, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/ssl.py\", line 911, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-35e4ef9beeda>\", line 39, in <module>\n",
      "    time.sleep(10)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n",
      "    if (yield from self.run_code(code, result)):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2036, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1379, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1282, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1144, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/posixpath.py\", line 428, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/posixpath.py\", line 75, in join\n",
      "    def join(a, *p):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-35e4ef9beeda>\", line 32, in <module>\n",
      "    proto_examples_i = utils.prep_cnn_np(survey_df_yeari_chunki, SATELLITE, KERNEL_SIZE, year_i)\n",
      "  File \"/Users/robmarty/Documents/Github/Pakistan-Poverty-from-Sky/DataWork/02_get_process_ancillary_data/CNN Features Predict NTL/ee_utils.py\", line 443, in prep_cnn_np\n",
      "    dict_ee = values_ee.getInfo()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/collection.py\", line 127, in getInfo\n",
      "    return super(Collection, self).getInfo()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/computedobject.py\", line 98, in getInfo\n",
      "    return data.computeValue(self)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/data.py\", line 678, in computeValue\n",
      "    prettyPrint=False))['result']\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/ee/data.py\", line 334, in _execute_cloud_call\n",
      "    return call.execute(num_retries=num_retries)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/_helpers.py\", line 134, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/http.py\", line 909, in execute\n",
      "    headers=self.headers,\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/googleapiclient/http.py\", line 177, in _retry_request\n",
      "    resp, content = http.request(uri, method, *args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/google_auth_httplib2.py\", line 198, in request\n",
      "    uri, method, body=body, headers=request_headers, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2/__init__.py\", line 1726, in request\n",
      "    conn, authority, uri, request_uri, method, body, headers, redirections, cachekey,\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2/__init__.py\", line 1441, in _request\n",
      "    (response, content) = self._conn_request(conn, request_uri, method, body, headers)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/httplib2shim/__init__.py\", line 148, in _conn_request\n",
      "    decode_content=decode)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 72, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 150, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\", line 324, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 1321, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 296, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/http/client.py\", line 257, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/ssl.py\", line 1052, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/ssl.py\", line 911, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-35e4ef9beeda>\", line 39, in <module>\n",
      "    time.sleep(10)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n",
      "    if (yield from self.run_code(code, result)):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2036, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1379, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1282, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1144, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/robmarty/anaconda3/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2032\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2033\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2034\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2034\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2035\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2036\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2038\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1379\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m             )\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m             \u001b[0mformatted_exceptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m             \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "# DONT SKIP ERRORS\n",
    "if True:\n",
    "    ### Loop through all tfrecords\n",
    "    for tfr_i in tf_record_list:\n",
    "\n",
    "        # Sometimes we get computational time out errors. If occurs, just skip and go to next.\n",
    "        # We can then go back and rescrape missed ones.\n",
    "\n",
    "        survey_df_yeari = survey_df[survey_df['tfrecord_name'] == tfr_i]\n",
    "        year_i = survey_df_yeari['year'].iloc[0]\n",
    "\n",
    "        ### Loop through chunks within tfrecord (can only pull so much data from GEE at a time)\n",
    "        survey_df_yeari['chunk_id'] = utils.chunk_ids(survey_df_yeari.shape[0], CHUNK_SIZE)\n",
    "\n",
    "        print(\"Putting \" + str(survey_df_yeari.shape[0]) + \" observations into \" + tfr_i)\n",
    "\n",
    "        proto_examples_all = []\n",
    "        for chunk_i in list(np.unique(survey_df_yeari.chunk_id)):\n",
    "            #time.sleep(6)\n",
    "            \n",
    "            ## Sometimes we hit a memory error; try until we don't hit that\n",
    "            \n",
    "            # TODO: could say: try 3 times?\n",
    "            try_extract_data = True\n",
    "            while try_extract_data:\n",
    "                try:\n",
    "\n",
    "                    print(\"Observation: \" + str(len(proto_examples_all)) + \"/\" + str(survey_df_yeari.shape[0]))\n",
    "\n",
    "                    survey_df_yeari_chunki = survey_df_yeari[survey_df_yeari['chunk_id'] == chunk_i]\n",
    "\n",
    "                    proto_examples_i = utils.prep_cnn_np(survey_df_yeari_chunki, SATELLITE, KERNEL_SIZE, year_i)\n",
    "                    proto_examples_all.extend(proto_examples_i)\n",
    "                    \n",
    "                    try_extract_data = False\n",
    "                    \n",
    "                except:\n",
    "                    print(\"Error!\")\n",
    "                    time.sleep(10)\n",
    "                    pass\n",
    "            \n",
    "        ### Save data as tf record\n",
    "        out_path_i = os.path.join(out_path, tfr_i)\n",
    "        print(out_path_i)\n",
    "        with tf.io.TFRecordWriter(out_path_i) as writer:\n",
    "            for tf_example in proto_examples_all:\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "\n",
    "        print(\"Success \\o/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP ERRORS\n",
    "\n",
    "### Loop through all tfrecords\n",
    "for tfr_i in tf_record_list:\n",
    "\n",
    "    # Sometimes we get computational time out errors. If occurs, just skip and go to next.\n",
    "    # We can then go back and rescrape missed ones.\n",
    "\n",
    "    survey_df_yeari = survey_df[survey_df['tfrecord_name'] == tfr_i]\n",
    "    year_i = survey_df_yeari['year'].iloc[0]\n",
    "\n",
    "    ### Loop through chunks within tfrecord (can only pull so much data from GEE at a time)\n",
    "    survey_df_yeari['chunk_id'] = utils.chunk_ids(survey_df_yeari.shape[0], CHUNK_SIZE)\n",
    "\n",
    "    print(\"Putting \" + str(survey_df_yeari.shape[0]) + \" observations into \" + tfr_i)\n",
    "\n",
    "    proto_examples_all = []\n",
    "    for chunk_i in list(np.unique(survey_df_yeari.chunk_id)):\n",
    "        ## Sometimes we hit a memory error; try until we don't hit that\n",
    "\n",
    "        # TODO: could say: try 3 times?\n",
    "        try_extract_data = 1\n",
    "        while try_extract_data < 4:\n",
    "            try:\n",
    "\n",
    "                print(\"Observation: \" + str(len(proto_examples_all)) + \"/\" + str(survey_df_yeari.shape[0]))\n",
    "\n",
    "                survey_df_yeari_chunki = survey_df_yeari[survey_df_yeari['chunk_id'] == chunk_i]\n",
    "\n",
    "                proto_examples_i = utils.prep_cnn_np(survey_df_yeari_chunki, SATELLITE, KERNEL_SIZE, year_i)\n",
    "                proto_examples_all.extend(proto_examples_i)\n",
    "                \n",
    "                try_extract_data = 10\n",
    "\n",
    "            except:\n",
    "                try_extract_data = try_extract_data + 1\n",
    "                print(\"Error!\")\n",
    "                print(try_extract_data)\n",
    "                print(survey_df_yeari_chunki['uid'])\n",
    "                time.sleep(5)\n",
    "                pass\n",
    "\n",
    "    ### Save data as tf record\n",
    "    out_path_i = os.path.join(out_path, tfr_i)\n",
    "    print(out_path_i)\n",
    "    with tf.io.TFRecordWriter(out_path_i) as writer:\n",
    "        for tf_example in proto_examples_all:\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print(\"Success \\o/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c89pBOUXZT_V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robmarty/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putting 36 observations into forcnn_UG_4_1_all.tfrecord\n",
      "Observation: 0/36\n",
      "Observation: 1/36\n",
      "Observation: 2/36\n",
      "Observation: 3/36\n",
      "Observation: 4/36\n",
      "Observation: 5/36\n",
      "Error ---\n",
      "2161    UG201800000328\n",
      "Name: uid, dtype: object\n",
      "Putting 246 observations into nocnn_IA_4_10_all.tfrecord\n",
      "Observation: 0/246\n",
      "Observation: 1/246\n",
      "Observation: 2/246\n",
      "Observation: 3/246\n",
      "Observation: 4/246\n",
      "Observation: 5/246\n",
      "Observation: 6/246\n",
      "Observation: 7/246\n",
      "Observation: 8/246\n",
      "Observation: 9/246\n",
      "Observation: 10/246\n",
      "Observation: 11/246\n",
      "Observation: 12/246\n",
      "Observation: 13/246\n",
      "Observation: 14/246\n",
      "Observation: 15/246\n",
      "Observation: 16/246\n",
      "Observation: 17/246\n",
      "Observation: 18/246\n",
      "Observation: 19/246\n",
      "Observation: 20/246\n",
      "Observation: 21/246\n",
      "Observation: 22/246\n",
      "Observation: 23/246\n",
      "Observation: 24/246\n",
      "Observation: 25/246\n",
      "Observation: 26/246\n",
      "Observation: 27/246\n",
      "Observation: 28/246\n",
      "Observation: 29/246\n",
      "Observation: 30/246\n",
      "Observation: 31/246\n",
      "Observation: 32/246\n",
      "Observation: 33/246\n",
      "Observation: 34/246\n",
      "Observation: 35/246\n",
      "Observation: 36/246\n",
      "Observation: 37/246\n",
      "Observation: 38/246\n",
      "Observation: 39/246\n",
      "Observation: 40/246\n",
      "Observation: 41/246\n",
      "Observation: 42/246\n",
      "Observation: 43/246\n",
      "Observation: 44/246\n",
      "Observation: 45/246\n",
      "Observation: 46/246\n",
      "Observation: 47/246\n",
      "Observation: 48/246\n",
      "Observation: 49/246\n",
      "Observation: 50/246\n",
      "Observation: 51/246\n",
      "Observation: 52/246\n",
      "Observation: 53/246\n",
      "Observation: 54/246\n",
      "Observation: 55/246\n",
      "Observation: 56/246\n",
      "Observation: 57/246\n",
      "Observation: 58/246\n",
      "Observation: 59/246\n",
      "Observation: 60/246\n",
      "Observation: 61/246\n",
      "Observation: 62/246\n",
      "Observation: 63/246\n",
      "Observation: 64/246\n",
      "Observation: 65/246\n",
      "Observation: 66/246\n",
      "Observation: 67/246\n",
      "Observation: 68/246\n",
      "Observation: 69/246\n",
      "Observation: 70/246\n",
      "Observation: 71/246\n",
      "Observation: 72/246\n",
      "Observation: 73/246\n",
      "Observation: 74/246\n",
      "Observation: 75/246\n",
      "Observation: 76/246\n",
      "Observation: 77/246\n",
      "Observation: 78/246\n",
      "Observation: 79/246\n",
      "Observation: 80/246\n",
      "Observation: 81/246\n",
      "Observation: 82/246\n",
      "Observation: 83/246\n",
      "Observation: 84/246\n",
      "Observation: 85/246\n",
      "Observation: 86/246\n",
      "Observation: 87/246\n",
      "Observation: 88/246\n",
      "Observation: 89/246\n",
      "Observation: 90/246\n",
      "Observation: 91/246\n",
      "Observation: 92/246\n",
      "Observation: 93/246\n",
      "Observation: 94/246\n",
      "Observation: 95/246\n",
      "Observation: 96/246\n",
      "Observation: 97/246\n",
      "Observation: 98/246\n",
      "Observation: 99/246\n",
      "Observation: 100/246\n",
      "Observation: 101/246\n",
      "Observation: 102/246\n",
      "Observation: 103/246\n",
      "Observation: 104/246\n",
      "Observation: 105/246\n",
      "Observation: 106/246\n",
      "Observation: 107/246\n",
      "Observation: 108/246\n",
      "Observation: 109/246\n",
      "Observation: 110/246\n",
      "Observation: 111/246\n",
      "Observation: 112/246\n",
      "Observation: 113/246\n",
      "Observation: 114/246\n",
      "Observation: 115/246\n",
      "Observation: 116/246\n",
      "Observation: 117/246\n",
      "Observation: 118/246\n",
      "Observation: 119/246\n",
      "Observation: 120/246\n",
      "Observation: 121/246\n",
      "Observation: 122/246\n",
      "Observation: 123/246\n",
      "Observation: 124/246\n",
      "Observation: 125/246\n",
      "Observation: 126/246\n",
      "Observation: 127/246\n",
      "Observation: 128/246\n",
      "Observation: 129/246\n",
      "Observation: 130/246\n",
      "Observation: 131/246\n",
      "Observation: 132/246\n",
      "Observation: 133/246\n",
      "Observation: 134/246\n",
      "Observation: 135/246\n",
      "Observation: 136/246\n",
      "Observation: 137/246\n",
      "Observation: 138/246\n",
      "Observation: 139/246\n",
      "Observation: 140/246\n",
      "Observation: 141/246\n",
      "Observation: 142/246\n",
      "Observation: 143/246\n",
      "Observation: 144/246\n",
      "Observation: 145/246\n",
      "Observation: 146/246\n",
      "Observation: 147/246\n",
      "Observation: 148/246\n",
      "Observation: 149/246\n",
      "Observation: 150/246\n",
      "Observation: 151/246\n",
      "Observation: 152/246\n",
      "Observation: 153/246\n",
      "Observation: 154/246\n",
      "Observation: 155/246\n",
      "Observation: 156/246\n",
      "Observation: 157/246\n",
      "Observation: 158/246\n",
      "Observation: 159/246\n",
      "Observation: 160/246\n",
      "Observation: 161/246\n",
      "Observation: 162/246\n",
      "Observation: 163/246\n",
      "Observation: 164/246\n",
      "Observation: 165/246\n",
      "Observation: 166/246\n",
      "Observation: 167/246\n",
      "Observation: 168/246\n",
      "Observation: 169/246\n",
      "Observation: 170/246\n",
      "Observation: 171/246\n",
      "Observation: 172/246\n",
      "Observation: 173/246\n",
      "Observation: 174/246\n",
      "Observation: 175/246\n",
      "Observation: 176/246\n",
      "Observation: 177/246\n",
      "Observation: 178/246\n",
      "Observation: 179/246\n",
      "Observation: 180/246\n",
      "Observation: 181/246\n",
      "Observation: 182/246\n",
      "Observation: 183/246\n",
      "Observation: 184/246\n",
      "Observation: 185/246\n",
      "Observation: 186/246\n",
      "Observation: 187/246\n",
      "Observation: 188/246\n",
      "Observation: 189/246\n",
      "Observation: 190/246\n",
      "Observation: 191/246\n",
      "Observation: 192/246\n",
      "Observation: 193/246\n",
      "Observation: 194/246\n",
      "Observation: 195/246\n",
      "Observation: 196/246\n",
      "Observation: 197/246\n",
      "Observation: 198/246\n",
      "Observation: 199/246\n",
      "Observation: 200/246\n",
      "Observation: 201/246\n",
      "Observation: 202/246\n",
      "Observation: 203/246\n",
      "Observation: 204/246\n",
      "Observation: 205/246\n",
      "Observation: 206/246\n",
      "Observation: 207/246\n",
      "Observation: 208/246\n",
      "Observation: 209/246\n",
      "Observation: 210/246\n",
      "Observation: 211/246\n",
      "Observation: 212/246\n",
      "Observation: 213/246\n",
      "Observation: 214/246\n",
      "Observation: 215/246\n",
      "Observation: 216/246\n",
      "Observation: 217/246\n",
      "Observation: 218/246\n",
      "Observation: 219/246\n",
      "Observation: 220/246\n",
      "Observation: 221/246\n",
      "Observation: 222/246\n",
      "Observation: 223/246\n",
      "Observation: 224/246\n",
      "Observation: 225/246\n",
      "Observation: 226/246\n",
      "Observation: 227/246\n",
      "Observation: 228/246\n",
      "Observation: 229/246\n",
      "Observation: 230/246\n",
      "Observation: 231/246\n",
      "Observation: 232/246\n",
      "Observation: 233/246\n",
      "Observation: 234/246\n",
      "Observation: 235/246\n",
      "Observation: 236/246\n",
      "Observation: 237/246\n",
      "Observation: 238/246\n",
      "Observation: 239/246\n",
      "Observation: 240/246\n",
      "Observation: 241/246\n",
      "Observation: 242/246\n",
      "Observation: 243/246\n",
      "Observation: 244/246\n",
      "Observation: 245/246\n",
      "/Users/robmarty/Google Drive/World Bank/IEs/Pakistan Poverty Estimation/Data/DHS/FinalData/Individual Datasets/cnn_s2/tfrecords/nocnn_IA_4_10_all.tfrecord\n",
      "Success \\o/\n",
      "Putting 56 observations into nocnn_UG_4_1_all.tfrecord\n",
      "Observation: 0/56\n",
      "Observation: 1/56\n",
      "Observation: 2/56\n",
      "Observation: 3/56\n",
      "Observation: 4/56\n",
      "Observation: 5/56\n",
      "Observation: 6/56\n",
      "Observation: 7/56\n",
      "Observation: 8/56\n",
      "Observation: 9/56\n",
      "Observation: 10/56\n",
      "Observation: 11/56\n",
      "Observation: 12/56\n",
      "Observation: 13/56\n",
      "Observation: 14/56\n",
      "Error ---\n",
      "43304    UG201800000332\n",
      "Name: uid, dtype: object\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfMKJ0ILXVZC",
    "outputId": "4c960a89-0028-4fdd-be62-5e7d0e073240"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "I6NhXtwo73AK",
    "outputId": "988398b3-4e62-4be2-8817-532af8d6f58e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   3408\u001b[0m         \"\"\"\n\u001b[0;32m-> 3409\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3410\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3394\u001b[0m         new_data = self._data.take(\n\u001b[0;32m-> 3395\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3396\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0725ac8cfe30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mproto_examples_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_cnn_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurvey_df_yeari_chunki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSATELLITE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKERNEL_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2127\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "for row_i in range(0,10):\n",
    "    print(row_i)\n",
    "    proto_examples_i = utils.prep_cnn_np(survey_df_yeari_chunki.iloc[[row_i]], SATELLITE, KERNEL_SIZE, year_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3l_U7gwb_vDC"
   },
   "outputs": [],
   "source": [
    "survey_df_yeari_chunki.iloc[[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "YuhpLiDq_zLm",
    "outputId": "f5dc6270-4493-4cbf-b2b0-18ec047077fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU05sc-s29jA",
    "outputId": "f9f946fc-d6a6-44e8-c2c2-539ec29937b9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVeX2_WIkGiV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "4RhcmrWylK6k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "pFHX00Y4ld81",
    "outputId": "4c9c913a-dce5-47b7-e28e-b9c6f14b7fed"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tk6RNyHXVZC",
    "outputId": "8806e5aa-370a-4a24-c6ba-062cf52f5ffb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAVqwXz7_lDL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9atvzCW65oh",
    "outputId": "515a1320-e759-4e12-a053-362d7562c52f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UrbRLqLKAtA",
    "outputId": "936ca132-44fc-45cb-d754-ad9dcfb8901e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaBcOr2w77ZL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l69Wpxz5XVZC",
    "outputId": "5d4d531b-61ef-45c7-b5b5-f25cb875c6b6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9MXwUdTXVZD",
    "outputId": "0bc2748f-ebe3-466d-a859-924eaa1b9081"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "iLbrGQTRojcS",
    "outputId": "43aabd48-e175-4d1c-f5f1-48be198520b0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "VHZAJ22anj6b",
    "outputId": "bebf3669-139a-423a-a967-78109a8c80a2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1MXAhkbpRkI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "93PWEi6jm-3K",
    "outputId": "67dcc0a7-1516-4ee8-bfd9-694fd0f347de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Rm6K6D-XVZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fod-VYxLXVZE",
    "outputId": "3dd1f56a-476b-4aad-8279-86840b0bc026"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0iUwZmAXVZE",
    "outputId": "a33e2442-ae0c-4f62-ce6b-eddb002c165d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62ReOurYXVZE",
    "outputId": "92795c41-e2b9-4971-f512-9d56b18cf5d8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SajCSlNXVZE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJAM4iswXVZE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wYDJxfpXVZF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zb0U780PXVZF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBuKts4wXVZF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IS7Bb2f0XVZF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju-_zuLnXVZF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fD0SacCEXVZF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xqAkzCqXVZG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDskpn-iXVZG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Iep8MNNXVZG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNjhMUDHXVZH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPzEHDBcXVZH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVI8BPYMXVZH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQR8aHw2XVZH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5pJ0FIOXVZI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9GuSI50opSX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_extract_data_gee_for_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
