{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poverty Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_NAME = \"OPM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import geopandas as gpd\n",
    "import json\n",
    "#import rasterio\n",
    "#from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer, StandardScaler, normalize\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (BaggingClassifier, AdaBoostClassifier,\n",
    "                              AdaBoostRegressor,\n",
    "                              GradientBoostingClassifier, RandomForestClassifier,\n",
    "                              RandomForestRegressor,\n",
    "                             BaggingRegressor, GradientBoostingRegressor)\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                             recall_score, classification_report,\n",
    "                            r2_score, mean_absolute_error, mean_squared_error)\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import logging, os \n",
    "\n",
    "import grid_params as grids\n",
    "import config as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = os.path.join(cf.DROPBOX_DIRECTORY, 'data', SURVEY_NAME, 'FinalData', 'pov_estimation_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(cf.DROPBOX_DIRECTORY, 'data', SURVEY_NAME, \"FinalData\", \"Merged Datasets\", \"survey_alldata_clean.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SURVEY_NAME == \"OPM\":\n",
    "    df = df[df.year == 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>year</th>\n",
       "      <th>survey_round</th>\n",
       "      <th>GID_3</th>\n",
       "      <th>GID_2</th>\n",
       "      <th>GID_1</th>\n",
       "      <th>NAME_3</th>\n",
       "      <th>NAME_2</th>\n",
       "      <th>NAME_1</th>\n",
       "      <th>...</th>\n",
       "      <th>fb_wp_estimate_mau_24</th>\n",
       "      <th>fb_wp_estimate_mau_25</th>\n",
       "      <th>fb_wp_estimate_mau_26</th>\n",
       "      <th>fb_wp_estimate_mau_27</th>\n",
       "      <th>fb_wp_estimate_mau_28</th>\n",
       "      <th>fb_wp_estimate_mau_29</th>\n",
       "      <th>fb_wp_estimate_mau_30</th>\n",
       "      <th>fb_wp_estimate_mau_31</th>\n",
       "      <th>fb_wp_estimate_mau_32</th>\n",
       "      <th>fb_wp_estimate_mau_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_PAK.7.7.4_1</td>\n",
       "      <td>U</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>PAK.7.7.4_1</td>\n",
       "      <td>PAK.7.7_1</td>\n",
       "      <td>PAK.7_1</td>\n",
       "      <td>Rawalpindi</td>\n",
       "      <td>Rawalpindi</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026854</td>\n",
       "      <td>0.022234</td>\n",
       "      <td>0.076231</td>\n",
       "      <td>0.047067</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.034073</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.040425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10_PAK.7.7.4_1</td>\n",
       "      <td>R</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>PAK.7.7.4_1</td>\n",
       "      <td>PAK.7.7_1</td>\n",
       "      <td>PAK.7_1</td>\n",
       "      <td>Rawalpindi</td>\n",
       "      <td>Rawalpindi</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298991</td>\n",
       "      <td>0.348720</td>\n",
       "      <td>1.074131</td>\n",
       "      <td>0.607928</td>\n",
       "      <td>0.031080</td>\n",
       "      <td>0.135510</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>0.904433</td>\n",
       "      <td>0.625955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100_PAK.7.3.2_1</td>\n",
       "      <td>R</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>PAK.7.3.2_1</td>\n",
       "      <td>PAK.7.3_1</td>\n",
       "      <td>PAK.7_1</td>\n",
       "      <td>Jhang</td>\n",
       "      <td>Faisalabad</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123656</td>\n",
       "      <td>0.052151</td>\n",
       "      <td>0.200538</td>\n",
       "      <td>0.098388</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>0.091936</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.156452</td>\n",
       "      <td>0.080646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>101_PAK.7.3.2_1</td>\n",
       "      <td>R</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>PAK.7.3.2_1</td>\n",
       "      <td>PAK.7.3_1</td>\n",
       "      <td>PAK.7_1</td>\n",
       "      <td>Jhang</td>\n",
       "      <td>Faisalabad</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078187</td>\n",
       "      <td>0.031331</td>\n",
       "      <td>0.124760</td>\n",
       "      <td>0.067461</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.019758</td>\n",
       "      <td>0.055323</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>0.097945</td>\n",
       "      <td>0.053348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>102_PAK.7.3.2_1</td>\n",
       "      <td>R</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>PAK.7.3.2_1</td>\n",
       "      <td>PAK.7.3_1</td>\n",
       "      <td>PAK.7_1</td>\n",
       "      <td>Jhang</td>\n",
       "      <td>Faisalabad</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018748</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>0.023748</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>0.014999</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>0.024998</td>\n",
       "      <td>0.012499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                uid urban_rural  year  survey_round        GID_3      GID_2  \\\n",
       "3     1_PAK.7.7.4_1           U  2014             3  PAK.7.7.4_1  PAK.7.7_1   \n",
       "7    10_PAK.7.7.4_1           R  2014             3  PAK.7.7.4_1  PAK.7.7_1   \n",
       "11  100_PAK.7.3.2_1           R  2014             3  PAK.7.3.2_1  PAK.7.3_1   \n",
       "14  101_PAK.7.3.2_1           R  2014             3  PAK.7.3.2_1  PAK.7.3_1   \n",
       "18  102_PAK.7.3.2_1           R  2014             3  PAK.7.3.2_1  PAK.7.3_1   \n",
       "\n",
       "      GID_1      NAME_3      NAME_2  NAME_1  ... fb_wp_estimate_mau_24  \\\n",
       "3   PAK.7_1  Rawalpindi  Rawalpindi  Punjab  ...              0.026854   \n",
       "7   PAK.7_1  Rawalpindi  Rawalpindi  Punjab  ...              0.298991   \n",
       "11  PAK.7_1       Jhang  Faisalabad  Punjab  ...              0.123656   \n",
       "14  PAK.7_1       Jhang  Faisalabad  Punjab  ...              0.078187   \n",
       "18  PAK.7_1       Jhang  Faisalabad  Punjab  ...              0.018748   \n",
       "\n",
       "    fb_wp_estimate_mau_25  fb_wp_estimate_mau_26  fb_wp_estimate_mau_27  \\\n",
       "3                0.022234               0.076231               0.047067   \n",
       "7                0.348720               1.074131               0.607928   \n",
       "11               0.052151               0.200538               0.098388   \n",
       "14               0.031331               0.124760               0.067461   \n",
       "18               0.012499               0.023748               0.012499   \n",
       "\n",
       "    fb_wp_estimate_mau_28  fb_wp_estimate_mau_29  fb_wp_estimate_mau_30  \\\n",
       "3                0.002888               0.011550               0.034073   \n",
       "7                0.031080               0.135510               0.457500   \n",
       "11               0.005376               0.031721               0.091936   \n",
       "14               0.003387               0.019758               0.055323   \n",
       "18               0.012499               0.012499               0.014999   \n",
       "\n",
       "    fb_wp_estimate_mau_31  fb_wp_estimate_mau_32  fb_wp_estimate_mau_33  \n",
       "3                0.010395               0.065258               0.040425  \n",
       "7                0.151050               0.904433               0.625955  \n",
       "11               0.024194               0.156452               0.080646  \n",
       "14               0.016089               0.097945               0.053348  \n",
       "18               0.012499               0.024998               0.012499  \n",
       "\n",
       "[5 rows x 336 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "#df = df[df.country_code != 'IA']\n",
    "#df = df[df.country_code != 'TL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602, 336)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmodel_valresult(df, country, est_type, target, parameters, feature_type):\n",
    "    # DESCRIPTION: Estimate poverty for a specific country, feature set and \n",
    "    # poverty variable across a number of parameters\n",
    "    # ARGS:\n",
    "    # df: Dataframe\n",
    "    # country: iso2\n",
    "    # est_type: within_country or other_countries\n",
    "    # target: dependent variable to predict\n",
    "    # feature_type: features to use\n",
    "\n",
    "    df = df.dropna(subset=[target])\n",
    "    df.reset_index()\n",
    "    \n",
    "    # Within country prediction; predict across folds within a country\n",
    "    if est_type == 'within_country_cv':\n",
    "        df_traintest = df[df.country_code == country]\n",
    "        df_traintest.reset_index()\n",
    "        \n",
    "        df_traintest['fold'] = df_traintest['within_country_fold']\n",
    "    else:\n",
    "        df['fold'] = 'fold_1'\n",
    "        \n",
    "    # Train model on all countries except country i; predict on country i\n",
    "    if est_type == 'global_country_pred':\n",
    "        df_traintest = df\n",
    "        df_traintest.reset_index()\n",
    "    \n",
    "    # Train model on all countries in a continent, except country i; predict\n",
    "    # on country x\n",
    "    if est_type == 'continent_africa_country_pred':\n",
    "        df_traintest = df[(df.country_code == country) | (df.continent_adj == 'Africa')]\n",
    "        df_traintest.reset_index()\n",
    "        \n",
    "    if est_type == 'continent_americas_country_pred':\n",
    "        df_traintest = df[(df.country_code == country) | (df.continent_adj == 'Americas')]\n",
    "        df_traintest.reset_index()\n",
    "        \n",
    "    if est_type == 'continent_eurasia_country_pred':\n",
    "        df_traintest = df[(df.country_code == country) | (df.continent_adj == 'Eurasia')]\n",
    "        df_traintest.reset_index()\n",
    "        \n",
    "    # Train on all countries in continent x and predict on countries in continent y\n",
    "    if est_type == 'continent':\n",
    "        df_traintest = df\n",
    "        df['fold'] = df['continent_adj']\n",
    "           \n",
    "    # Initialize results and predictions dataframes\n",
    "    results_df = pd.DataFrame() # results iterating over params\n",
    "    \n",
    "    y_df = df_traintest[['uid', 'country_code']]\n",
    "    y_df = y_df.copy()\n",
    "    y_df['y'] = df_traintest[target]\n",
    "    #y_df['y'] = df_traintest.loc[:, (target)]\n",
    "    y_df['target'] = target\n",
    "    y_df['feature_type'] = feature_type\n",
    "    y_df['est_type'] = est_type\n",
    "    \n",
    "    model_i = 0\n",
    "    for i in parameters['regressors']:\n",
    "        for j in parameters[i]:\n",
    "            \n",
    "            pred_dict = {\n",
    "                'regressor': i,\n",
    "                'params': j,\n",
    "                'country': country,\n",
    "                'est_type': est_type,\n",
    "                'target': target,\n",
    "                'model_i': model_i,\n",
    "                'feature_type': feature_type\n",
    "             }\n",
    "\n",
    "            fold = 0\n",
    "            y_df_parami = pd.DataFrame()\n",
    "            for split_id in df_traintest.fold.unique():\n",
    "                \n",
    "                #### Separate into train and test\n",
    "                if 'country_pred' in est_type:\n",
    "                    df_train = df_traintest[df_traintest.country_code != country]\n",
    "                    df_test = df_traintest[df_traintest.country_code == country]\n",
    "                elif est_type == 'continent':\n",
    "                    df_train = df_traintest[df_traintest['fold'] == split_id]\n",
    "                    df_test = df_traintest[df_traintest['fold'] != split_id]\n",
    "                else:\n",
    "                    df_train = df_traintest[df_traintest['fold'] != split_id]\n",
    "                    df_test = df_traintest[df_traintest['fold'] == split_id]\n",
    "\n",
    "                #### Select features\n",
    "                if (feature_type == \"fb\"):\n",
    "                    x_train = df_train.filter(regex='^fb_', axis=1)\n",
    "                    x_test = df_test.filter(regex='^fb_', axis=1)\n",
    "                    \n",
    "                if (feature_type == \"osm\"):\n",
    "                    x_train = df_train.filter(regex='^osm_', axis=1)\n",
    "                    x_test = df_test.filter(regex='^osm_', axis=1)\n",
    "                    \n",
    "                if (feature_type == \"gc_\"):\n",
    "                    x_train = df_train.filter(regex='^gc_', axis=1)\n",
    "                    x_test = df_test.filter(regex='^gc_', axis=1)\n",
    "                    \n",
    "                if (feature_type == \"l8\"):\n",
    "                    x_train = df_train.filter(regex='^l8_', axis=1)\n",
    "                    x_test = df_test.filter(regex='^l8_', axis=1)\n",
    "                    \n",
    "                if (feature_type == \"l8_viirs\"):\n",
    "                    x_train = df_train.filter(regex='^l8_|^viirs_', axis=1)\n",
    "                    x_test = df_test.filter(regex='^l8_|^viirs_', axis=1)\n",
    "                    \n",
    "                if (feature_type == \"all\"):\n",
    "                    x_train = df_train.filter(regex ='^fb_|^l8_|^viirs_|^osm_|^worldpop_|^gc_', axis=1)\n",
    "                    x_test  = df_test.filter(regex  ='^fb_|^l8_|^viirs_|^osm_|^worldpop_|^gc_', axis=1)\n",
    "                    \n",
    "                #### Prep Y Data\n",
    "                y_train = df_train[target]\n",
    "                y_test = df_test[target]\n",
    "\n",
    "                #### Prep X Data: Normalize\n",
    "                x_scaler = StandardScaler().fit(x_train)\n",
    "\n",
    "                x_train = x_scaler.transform(x_train)\n",
    "                x_test = x_scaler.transform(x_test)\n",
    "\n",
    "                ### Initialize regressor, fit data, then append model to list\n",
    "                regressor = eval(i)(**j)\n",
    "                trained = regressor.fit(x_train, y_train)\n",
    "\n",
    "                ### Results\n",
    "                y_pred = trained.predict(x_test)\n",
    "                \n",
    "                pred_dict['r2_score_' + str(split_id)] = r2_score(y_test, y_pred)\n",
    "                \n",
    "                y_dict_foldi = {\n",
    "                    'uid': df_test.uid\n",
    "                 }\n",
    "                y_dict_foldi['y_' + str(model_i)] = y_pred\n",
    "                \n",
    "                y_df_foldi = pd.DataFrame.from_dict(y_dict_foldi)\n",
    "                y_df_parami = y_df_parami.append(y_df_foldi, ignore_index=True)\n",
    "\n",
    "                fold += 1\n",
    "\n",
    "            y_df = y_df.merge(y_df_parami, on = 'uid', how = 'right')\n",
    "            y_df.to_csv(os.path.join(OUT_DIR, 'individual_files',\n",
    "                               'withincv_predicted_values_' + \n",
    "                               country + '_' +\n",
    "                               est_type + '_' +\n",
    "                               feature_type + '_' +\n",
    "                               target + \n",
    "                               '.csv'))\n",
    "                \n",
    "            results_df = results_df.append(pred_dict, ignore_index=True)\n",
    "            results_df.to_csv(os.path.join(OUT_DIR, 'individual_files',\n",
    "                                           'withincv_results_' + \n",
    "                                           country + '_' +\n",
    "                                           est_type + '_' +\n",
    "                                           feature_type + '_' +\n",
    "                                           target + \n",
    "                                           '.csv'))\n",
    "            model_i += 1\n",
    "        \n",
    "    return results_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = grids.GRID_REGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SURVEY_NAME == 'DHS':\n",
    "    target_vars = ['pca_allvars']\n",
    "elif SURVEY_NAME == 'OPM':\n",
    "    target_vars = ['pscores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "osm // within_country_cv // PK // pscores\n",
      "fb // within_country_cv // PK // pscores\n",
      "all // within_country_cv // PK // pscores\n"
     ]
    }
   ],
   "source": [
    "results_all_df = pd.DataFrame()\n",
    "y_all_df = pd.DataFrame()\n",
    "\n",
    "# 'fb', 'osm', 'l8', 'l8_viirs', 'all'\n",
    "for feature_type_i in ['osm', 'fb', 'all']:\n",
    "    for est_type_i in ['continent', 'continent_americas_country_pred', 'within_country_cv']:\n",
    "        for cc_i in df.country_code.unique():\n",
    "            for target in target_vars: # 'wealth_index_score'\n",
    "                \n",
    "                # For OPM survey, only do within_country_cv\n",
    "                if SURVEY_NAME == \"OPM\":\n",
    "                    if est_type_i != \"within_country_cv\":\n",
    "                        continue\n",
    "                      \n",
    "                # For predicting country i using continent x, only use\n",
    "                # other countries in the continent.\n",
    "                if est_type_i == 'continent_africa_country_pred':\n",
    "                    continent_i = df.continent_adj[df.country_code == cc_i].tolist()[0]\n",
    "                    if continent_i != 'Africa':\n",
    "                        continue\n",
    "                        \n",
    "                if est_type_i == 'continent_americas_country_pred':\n",
    "                    continent_i = df.continent_adj[df.country_code == cc_i].tolist()[0]\n",
    "                    if continent_i != 'Americas':\n",
    "                        continue\n",
    "                        \n",
    "                if est_type_i == 'continent_eurasia_country_pred':\n",
    "                    continent_i = df.continent_adj[df.country_code == cc_i].tolist()[0]\n",
    "                    if continent_i != 'Eurasia':\n",
    "                        continue\n",
    "                        \n",
    "                # Only need to run 'continent' once\n",
    "                if est_type_i == 'continent':\n",
    "                    if cc_i != 'AL':\n",
    "                        continue\n",
    "                \n",
    "                # Print where at\n",
    "                print(feature_type_i + ' // ' + est_type_i + ' // ' + cc_i + ' // ' + target)\n",
    "\n",
    "                # Run ML Model\n",
    "                results_df_i, y_df_i = trainmodel_valresult(df, cc_i, est_type_i, target, parameters, feature_type_i)\n",
    "\n",
    "                # Grab results\n",
    "                results_all_df = results_all_df.append(results_df_i, ignore_index=True)\n",
    "                y_all_df = y_all_df.append(y_df_i, ignore_index=True)\n",
    "                \n",
    "                # Export CSVs; replace as process more models\n",
    "                results_all_df.to_csv(os.path.join(OUT_DIR, 'results_fbonly_withincv.csv'))\n",
    "                y_all_df.to_csv(os.path.join(OUT_DIR, 'ypred_fbonly_withincv.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_df.to_csv(os.path.join(OUT_DIR, 'results_fbonly_withincv.csv'))\n",
    "y_all_df.to_csv(os.path.join(OUT_DIR, 'ypred_fbonly_withincv.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_type = 'hello_country_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'country_pred' in est_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
